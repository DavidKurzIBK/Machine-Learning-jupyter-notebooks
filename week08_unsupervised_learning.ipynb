{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"week08_unsupervised_learning.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1252f59b9d7d51edfad9ef1a6b8cf5bb","grade":false,"grade_id":"cell-268562de23705a50","locked":true,"schema_version":3,"solution":false,"task":false},"id":"IeNNbKpz2ZMz","colab_type":"text"},"source":["# Overview  <a name='objectives' />\n","\n","This notebook introduces you to a few Unsupervised Learning methods.\n","\n","The topics that will be covered are:\n","\n","1. <a href=#k_means>Naive k-Means</a>\n","2. <a href=#fuzzy_k_means>Fuzzy k-Means</a>\n","3. <a href=#agglomerative_clustering>Agglomerative Clustering</a>\n","\n","### Programming Tasks\n","For the programming tasks you will need to replace the following comment and exception with your own code:\n","\n","```python\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","```\n","\n","Most programming tasks are followed by a cell with tests (using the `assert` function from python). You can use these cells while developing your implementation and for validating your implementation.\n","\n","**<font size=\"3\" color=\"red\">Note</font>**: The `@contract` decorators make sure the data types and shapes are correct for the inputs and outputs. See [here](https://andreacensi.github.io/contracts/tour.html#quick-tour) for more. If you are more comfortable working without these, you can comment out the lines starting with `@contract`. However, in that case it can get tedious to locate the exact source of a bug.\n","\n","### Open Questions\n","The notebook also contains a few open questions. For the open questions you can put your answer in the cell below the question."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d38d20d3bdcc782b39bd0fd71f28d94b","grade":false,"grade_id":"cell-e79238def5d85573","locked":true,"schema_version":3,"solution":false,"task":false},"id":"iV-zgJ6b2ZM0","colab_type":"code","colab":{}},"source":["# DO NOT INSTALL THE LIBRARIES WHEN WORKING ON ifi-europa.uibk.ac.at\n","\n","# Make sure that the required libraries are installed\n","# If you are using Google Colab, remember to upload the requirements file before \n","# running this cell\n","# If you are running this notebook locally, the requirements file needs to be in \n","# the same location as this notebook\n","import os\n","running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n","    \n","if running_local:\n","    import sys\n","    !{sys.executable} -m pip install -r requirements_week08.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ae17bb9489a0a65a963b5ecdf4805943","grade":false,"grade_id":"cell-6171abea0e27b546","locked":true,"schema_version":3,"solution":false,"task":false},"id":"oucED7wh2ZM3","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","### Setup"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9647952988353db0223cce0909c9a075","grade":false,"grade_id":"cell-b403d173576d6c36","locked":true,"schema_version":3,"solution":false,"task":false},"id":"82s8W5Tz2ZM3","colab_type":"code","colab":{}},"source":["%matplotlib notebook\n","\n","from collections import namedtuple\n","import time\n","from IPython.display import set_matplotlib_formats\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.cluster import hierarchy\n","from sklearn.datasets import make_blobs\n","from contracts import contract\n","\n","set_matplotlib_formats('svg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4aa6c1caf919081b6cdb7fd3baefc768","grade":false,"grade_id":"cell-ac41c62ebe255cf4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2quPSugt2ZM5","colab_type":"code","colab":{}},"source":["# Counter for figures\n","figcount = 0\n","\n","# Set the random seed for reproducing results\n","random_seed = 3\n","np.random.seed(random_seed)\n","\n","# Set colors\n","colors = plt.cm.tab10.colors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f63aae9da2ef7ff2023412d08213ed09","grade":false,"grade_id":"cell-a3bd9e0beb93fb65","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Cz1OfJhw2ZM8","colab_type":"code","colab":{}},"source":["def plot_data(ax, X, labels=None, centroids=None):\n","    \"\"\"Helper function for plotting data set and clustering result\n","    \n","    :param ax: Matplotlib Axes in which to plot the figure\n","    :param X: Dataset\n","    :param labels: Cluster labels for each data point in X\n","    :param centroids: Center of each cluster\n","    \"\"\"\n","    \n","    # Plot all data points\n","    if labels is None:\n","        ax.scatter(X[:, 0], X[:, 1], \n","                   s=30,\n","                   color=colors[0],\n","                   label=r'Data')\n","    # Plot data points with assigned cluster\n","    else:\n","        for label in range(min(labels), max(labels) + 1):\n","            ax.scatter(X[labels == label, 0], X[labels == label, 1], \n","                   s=20,\n","                   color=colors[label],\n","                   alpha=0.5,\n","                   label=r'Cluster {}'.format(label))\n","            \n","    # Plot cluster means and contour\n","    if centroids is not None:\n","        n_clusters = centroids.shape[0]\n","        \n","        # Plot means\n","        for label in range(n_clusters):\n","            if label == 0:\n","                kwargs = {\n","                    'label': 'centroids'\n","                }\n","            else:\n","                kwargs = {}\n","            ax.scatter(centroids[label, 0], centroids[label, 1],\n","                       s=100,\n","                       marker='X',\n","                       color='black',\n","                       **kwargs)\n","            \n","        # Plot contours\n","        xlim = ax.get_xlim()\n","        ylim = ax.get_ylim()\n","        \n","        # create grid to evaluate model\n","        xx = np.linspace(xlim[0], xlim[1], 100)\n","        yy = np.linspace(ylim[0], ylim[1], 100)\n","        YY, XX = np.meshgrid(yy, xx)\n","        xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","\n","        Z = assign_datapoints_to_cluster(xy, centroids)\n","        Z = np.asarray(Z).reshape(XX.shape)\n","\n","        ax.contourf(XX, YY, Z, alpha=0.1, colors=colors, levels=n_clusters-1)\n","    \n","    # Set Labels and Limits\n","    ax.set_xlabel(r'$x_0$')\n","    ax.set_ylabel(r'$x_1$')\n","    ax.set_xlim(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1)\n","    ax.set_ylim(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1)\n","    \n","    # Legend\n","    pst = ax.legend(loc='upper left', frameon=True)\n","    pst.get_frame().set_edgecolor('k')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4fcc324d6d34feaca95fde63c7b2346f","grade":false,"grade_id":"cell-920f8c1ebaf280ea","locked":true,"schema_version":3,"solution":false,"task":false},"id":"lQLwkpfK2ZM-","colab_type":"text"},"source":["We will be using the `make_blobs` function from scikit-learn to generate a dataset. In this case we know the number of clusters, but in reality you often do not. Later on in the exercise, we want to see if our k-Means algorithm is able to find the optimal number of centers for clustering the data."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f6cf9f5871abd0b2a91aeb56f6f5bef2","grade":false,"grade_id":"cell-ad7e285c76ae3e78","locked":true,"schema_version":3,"solution":false,"task":false},"id":"oA0JyK1O2ZM-","colab_type":"code","colab":{}},"source":["# Create dataset\n","blobs_centers = [[-1,-1], [1,-1], [0,1], [1, 1.5]]\n","blobs, _ = make_blobs(1000, centers=blobs_centers, cluster_std=0.5)\n","\n","# Plot dateset\n","plt.figure()\n","ax = plt.gca()\n","plot_data(ax, blobs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0cdf99923fae34928025343ff831fa7b","grade":false,"grade_id":"cell-967707ffdb9d66fb","locked":true,"schema_version":3,"solution":false,"task":false},"id":"gcS3vPvT2ZNA","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","# Part 1: Naive k-Means <a name='k_means' />\n","The *Naive k-Means* (or just *k-Means*) algorithm is a commonly used clustering algorithm, one of the reasons for that is its simplicity. The *Naive k-Means* algorithm requires us to define the number of clusters $k$ beforehand. The algorithm consist of the following steps:\n","\n","1. Initialize the center for each of the $k$ clusters (also called *centroids*).\n","2. Assign each datapoint in your dataset to the closest cluster.\n","3. Update the center of each cluster.\n","4. Calculate the sum of squared errors (SSE) for all the clusters (also known as *distortion*).\n","5. Evaluate whether the algorithm has converged (SSE has stabalised), if not return to step 2."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"63cb10a14885cde1e5ab104b592f44c5","grade":false,"grade_id":"cell-eceb1af71ff3008e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"3KSwcOBC2ZNA","colab_type":"text"},"source":["### Task 1.1: Initializing cluster centers\n","Choosing the right initial centers is important, if the wrong centers are chosen the algorithm might take very long to converge or not converge at all. An easy but effective way for initializing the centers is by sampling $k$ random datapoint from you dataset and assign them as initial cluster centers."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2ae29bc665b1b717d8304e7bc930361a","grade":false,"grade_id":"cell-9ede51edea983bb3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"eNgTHakI2ZNB","colab_type":"code","colab":{}},"source":["@contract(X='array[AxB], A>0, B>0', \n","          k='int, >0', \n","          returns='array[CxB], C>0, B>0')\n","def init_cluster_centers(X, k):\n","    \"\"\"Initialize centers of clusters\n","    \n","    Initialize the center of each cluster\n","    by selecting a random data point from X. \n","    \n","    Each row in the return centers will represent a\n","    cluster center and the row index represents the \n","    label of that cluster. For example:\n","    \n","      centroids = np.array([[1, 2],\n","                            [4, 5]])\n","                           \n","      - Cluster 0 is represented by the first row and its\n","        center is np.array([1, 2])\n","      - Cluster 1 is represented by the second row and\n","        its center is np.array([4, 5])\n","    \n","    \n","    :param X: Dataset (numpy array)\n","    :param k: Number of clusters (int)\n","    :param centers: The centers of the clusters (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return centers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6535b434efa6d13d061351809e141dd2","grade":true,"grade_id":"cell-3c995dc3b2417be4","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"0Z1kWvS62ZND","colab_type":"code","colab":{}},"source":["# Test with random data\n","X = np.random.uniform(-10, 10, size=(50, 4))\n","k = 3\n","\n","assert init_cluster_centers(X, k).shape == (k, X.shape[1])\n","\n","# Test different number of clusters\n","k = 4\n","assert init_cluster_centers(blobs, k).shape == (k, blobs.shape[1])\n","\n","\n","# Test on blobs dataset\n","assert init_cluster_centers(blobs, k).shape == (k, blobs.shape[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fec45192557581ca7f4131498cb87337","grade":false,"grade_id":"cell-f68ebde5f87c35ca","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NX4SL5Tv2ZNF","colab_type":"text"},"source":["### Task 1.2: Computing distances to cluster\n","Next we want to compte the distance of each datapoint in $\\mathbf{X}$ to each cluster center using the *squared euclidean distance*. For two datapoint $\\mathbf{x} = [x_0,\\ x_1,\\ \\ldots,\\ x_M]$ and $\\mathbf{y} = [y_0,\\ y_1,\\ \\ldots,\\ y_M]$ the squared euclidean distance is defined as:\n","\n","$$d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|^2 = (\\mathbf{x} - \\mathbf{y})^T (\\mathbf{x} - \\mathbf{y}) = \\sum_{i=0}^M(x_i - y_i)^2$$"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3613ba41bddbaccf47efee660a1ea198","grade":false,"grade_id":"cell-8e5099e2f4869b75","locked":false,"schema_version":3,"solution":true,"task":false},"id":"YwyTKIdl2ZNF","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          center='array[M] | array[1xM], M>0',\n","          returns='array[N], N>0')\n","def compute_distance_to_cluster(X, center):\n","    \"\"\"Calculate squared euclidean distance between datapoint in X and a cluster\n","    \n","    :param X: Dataset (numpy array)\n","    :param center: Center of one cluster (numpy array)\n","    :returns: distance of each datapoint to the centroid (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return dists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"858a76bbce8f384fa52d6559428c0a6f","grade":true,"grade_id":"cell-5b5fecaa382641f8","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"xp5JfafV2ZNH","colab_type":"code","colab":{}},"source":["# Test on some random data\n","X = np.ones((5,2))\n","center = np.zeros((1,2))\n","dists = compute_distance_to_cluster(X, center)\n","\n","assert dists.shape == (X.shape[0],)\n","assert np.allclose(dists, np.ones(X.shape[0]) * 2.0)\n","\n","\n","# Test on blobs dataset\n","assert compute_distance_to_cluster(blobs, blobs[0,:]).shape == (blobs.shape[0],)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a0d2fc1d673d23a87ab7a4279a944ac3","grade":false,"grade_id":"cell-b66a5a1b1724a82a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"IE76HkzY2ZNJ","colab_type":"text"},"source":["### Task 1.3: Assign datapoint to cluster\n","Assign each datapoint in $\\mathbf{X}$ to a cluster based on the shortest distance between the datapoint and the cluster center. Use the `compute_distance_to_cluster` function defined above."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2fde7772e56d07925e6214fcd501fe3f","grade":false,"grade_id":"cell-a8b8b4f2ca6c76b7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"PkbjVQDf2ZNJ","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          centers='array[KxM], K>0, M>0',\n","          returns='array[N], N>0')\n","def assign_datapoints_to_cluster(X, centers):\n","    \"\"\"Assign each datapoint in X to a cluster\n","    \n","    Remember that each row in centers represents a cluster\n","    and the label of that cluster is the row index.\n","    \n","    :param X: Dataset (numpy array)\n","    :param centers: Centers of the clusters (numpy array)\n","    :returns: labels of assigned cluster (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0278bc2d746e16d82e1ae1a65de04c7c","grade":true,"grade_id":"cell-1e20ee755b922a51","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"6dAZLA1Q2ZNL","colab_type":"code","colab":{}},"source":["# Test on some random data\n","X = np.vstack([np.ones((5,2)), np.zeros((5,2))])\n","centers = np.array([[1, 1], [0, 0]])\n","labels = assign_datapoints_to_cluster(X, centers)\n","\n","assert labels.shape == (X.shape[0],)\n","assert np.allclose(labels[:5], 0)\n","assert np.allclose(labels[-5:], 1)\n","\n","\n","# Test on blobs dataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","labels = assign_datapoints_to_cluster(blobs, centers)\n","assert labels.shape == (blobs.shape[0],)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cea1633dfd3d59d505698ee8e51c6fb1","grade":false,"grade_id":"cell-b9b19ac00d38d908","locked":true,"schema_version":3,"solution":false,"task":false},"id":"hNTiKeGj2ZNN","colab_type":"text"},"source":["### Task 1.4: Compute new cluster centers\n","Next we can compute a new center of each cluster by taking the mean of all the datapoints in that cluster.\n","\n","$$\\mathbf{y}_k = \\frac{1}{|X_k|}\\sum_{\\mathbf{x} \\in X_k}\\mathbf{x}$$\n","\n","where $\\mathbf{y}_k$ is the new center for cluster $k$, $X_k$ is the set of datapoints in cluster $k$ and $|X_k|$ is the number of datapoints in cluster $k$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3a84f86853e12c8bc9d6ee958e7a4515","grade":false,"grade_id":"cell-38963ad2e7d02f57","locked":false,"schema_version":3,"solution":true,"task":false},"id":"mHGkG-Ph2ZNN","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          labels='array[N], N>0',\n","          centers='array[KxM], K>0, M>0',\n","          returns='array[KxM], K>0, M>0')\n","def update_centers(X, labels, centers):\n","    \"\"\"Compute new cluster centers\n","    \n","    Note: It might be the case that a cluster has not been assigned,\n","          i.e., |X_k| = 0. This means we cannot compute a new center\n","          for this cluster.\n","          \n","          Think of a solution to this problem and implement it.\n","    \n","    :param X: Dataset (numpy array)\n","    :param labels: Cluster label of each datapoint in X (numpy array)\n","    :param centers: Cluster centers (numpy array)\n","    :param k: Number of clusters (int)\n","    :returns: New cluster centers (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return new_centers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"02d4df69e75833facfa74ab7e6e4011f","grade":true,"grade_id":"cell-7da14f855f66de0b","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"IyEKJQQv2ZNP","colab_type":"code","colab":{}},"source":["# Test on some random data\n","X = np.vstack([np.ones((5,2)), np.zeros((5,2))])\n","centers = np.array([[2.0, 2.0], [-1.0, -1.0]])\n","labels = assign_datapoints_to_cluster(X, centers)\n","\n","new_centers = update_centers(X, labels, centers)\n","assert new_centers.shape == centers.shape\n","assert np.allclose(new_centers, np.array([[1., 1.], [0., 0.]]))\n","\n","# Test with an empty cluster\n","X = np.random.uniform(-5., 5., size=(10, 2))\n","centers = np.array([[1, 2], [-1, 2], [0.4, 4]])\n","labels = np.random.choice([0, 1], size=(10,)) # Note that cluster label 2 will not occur in labels\n","\n","new_centers = update_centers(X, labels, centers)\n","assert not np.isnan(new_centers).any()\n","\n","\n","# Test on blobs datadataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","labels = assign_datapoints_to_cluster(blobs, centers)\n","\n","new_centers = update_centers(blobs, labels, centers)\n","assert new_centers.shape == centers.shape\n","assert not np.allclose(new_centers, centers)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ed9c754567cb09279aed31cf97f9f814","grade":false,"grade_id":"cell-82ca86e64deedd14","locked":true,"schema_version":3,"solution":false,"task":false},"id":"xwKhS0lQ2ZNT","colab_type":"text"},"source":["### Task 1.5: Computing the Sum of Squared Errors (SSE)\n","The *distortion* is measured as the sum of the distance between each datapoint and its closest cluster center, i.e., the sum of squared errors (SSE):\n","\n","$$\\text{SSE} = \\sum_{i=0}^K\\sum_{\\mathbf{x}\\in X_i}\\|\\mathbf{x} - \\mathbf{y}_i\\|^2$$\n","\n","where $K$ is the number of clusters, $X_i$ is the set of datapoints in cluster $i$ and $y_i$ is the center of cluster $i$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6d1380365f5c4a74bb0f287ef43872e5","grade":false,"grade_id":"cell-c939c379a11bc76d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"A_b-PU_M2ZNT","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          labels='array[N], N>0',\n","          centers='array[KxM], K>0, M>0',\n","          returns='float, >=0')\n","def compute_sse(X, labels, centers):\n","    \"\"\"Compute Sum of Squared Errors\n","    \n","    Note: You can use your compute_distance_to_cluster function.\n","    \n","    Note: It might be the case that a cluster has not been assigned,\n","          i.e., |X_i| = 0. This means we cannot compute a new center\n","          for this cluster.\n","          \n","          Think of a solution to this problem and implement it.\n","    \n","    :param X: Dataset (numpy array)\n","    :param labels: Cluster label of each datapoint in X (numpy array)\n","    :param centers: Cluster centers (numpy array)\n","    :param returns: SSE (float)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","        \n","    return sse"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"24e20feaf5a9b86af73ab1b4d6556ea5","grade":true,"grade_id":"cell-c0844174f051c5e8","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"T9JPbyWk2ZNV","colab_type":"code","colab":{}},"source":["# Test on some random data\n","k = 2\n","X = np.vstack([np.ones((5,2)), np.zeros((5,2))])\n","centers = np.array([[2.0, 2.0], [-1.0, -1.0]])\n","labels = assign_datapoints_to_cluster(X, centers)\n","\n","sse = compute_sse(X, labels, centers)\n","assert sse >= 0\n","assert abs(sse - 20.0) < 1e-5\n","\n","centers = np.array([[1.0, 1.0], [0.0, 0.0]])\n","labels = assign_datapoints_to_cluster(X, centers)\n","\n","sse = compute_sse(X, labels, centers)\n","assert abs(sse) < 1e-5\n","\n","\n","# Test on blobs dataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","labels = assign_datapoints_to_cluster(blobs, centers)\n","\n","sse = compute_sse(blobs, labels, centers)\n","assert sse >= 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c13a3a9dddf8a5e240bf5b73742a5dd0","grade":false,"grade_id":"cell-0dffc8a7efe8e7d9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"IcSCJSZz2ZNX","colab_type":"text"},"source":["### Task 1.6: Determine whether the algorithm has converged\n","k-Means is an iterative algorithm and we need a way to determine when to stop. There are two approaches, the first one is running the algorithm for a pre-defined number of iterations and use the final result. The second approach is stopping when the algorithm has converged. For naive k-Means this means stopping when the SSE does not change anymore between iterations (more precisely, changes less than some threshold).\n","\n","Often a combination of both approaches is used, without the limiting the maximum number of iteration the algorithm could hypothetically go on forever."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0aaf34621736fc2017aa59c110263a22","grade":false,"grade_id":"cell-10cd69b3f3ab6fe3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"8z1OfNAU2ZNY","colab_type":"code","colab":{}},"source":["@contract(prev_sse='float, >=0',\n","          cur_sse='float, >=0',\n","          threshold='float, >0')\n","def is_converged(prev_sse, cur_sse, threshold):\n","    \"\"\"Check whether the algorithm has converged\n","    \n","    The algorithm has converged when the difference\n","    in SSE is smaller then some threshold.\n","    \n","    :param prev_sse: SSE of previous iteration (float)\n","    :param cur_sse: SSE of current iteration (float)\n","    :param threshold: Threshold for comparing SSE's (float)\n","    :param returns: True if converged, False otherwise (bool)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","        \n","    return converged"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d74ffbb4d59f2813ca9a07d18e1e8206","grade":true,"grade_id":"cell-c1490593c5e9f828","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"XIJ6KlT22ZNa","colab_type":"code","colab":{}},"source":["# Test on some random data\n","threshold = 1e-5\n","k = 2\n","X = np.vstack([np.ones((5,2)), np.zeros((5,2))])\n","centers = np.array([[2.0, 2.0], [-1.0, -1.0]])\n","labels = assign_datapoints_to_cluster(X, centers)\n","prev_sse = compute_sse(X, labels, centers)\n","centers = update_centers(X, labels, centers)\n","cur_sse = compute_sse(X, labels, centers)\n","\n","assert is_converged(prev_sse, cur_sse, threshold) == False\n","assert is_converged(cur_sse, cur_sse, threshold) == True\n","\n","\n","# Test on blobs dataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","labels = assign_datapoints_to_cluster(blobs, centers)\n","prev_sse = compute_sse(blobs, labels, centers)\n","centers = update_centers(blobs, labels, centers)\n","cur_sse = compute_sse(blobs, labels, centers)\n","\n","assert is_converged(prev_sse, cur_sse, threshold) == False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"6eec3f5af29b2031c61f9020b4d0651b","grade":false,"grade_id":"cell-731472d6d7a67e59","locked":true,"schema_version":3,"solution":false,"task":false},"id":"6MFaWYLc2ZNb","colab_type":"text"},"source":["### Task 1.7: Combining all the parts\n","Now it is time to combine all the parts together. We have supplied a helper class for visualizing the progress of k-Means as an animation, this is a nice way of seeing how the clusters change each iteration. After that you will find the cell with the `k-means` function, which you have to implement."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"251042b213cce1c3d95bfef02504e5d3","grade":false,"grade_id":"cell-769030d51d8ea80d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"MUWlXy9e2ZNc","colab_type":"code","colab":{}},"source":["class PlotKMeans:\n","    \"\"\"Helper class for visualizing k-Means\n","    \n","    This class can be used to visualize the progress of our k-Means algorithm.\n","    Each iteration the figure can be updated with the new results.\n","    \n","    The parameter `sleep_time` can be used to increase or decrease the speed\n","    of the animation. If you put it to low you will not be able to see the changes\n","    but the learning will be doen faster.\n","    \"\"\"\n","    \n","    def __init__(self, title, sleep_time=0.1):\n","        \"\"\"\n","        :param title: Title to show above the figure (str)\n","        :param sleep_time: Amount of time to sleep between each iteration (float)\n","        \"\"\"\n","        self.sleep_time = sleep_time        \n","        self.itrs = []\n","        self.sses = []\n","        \n","        self.fig, self.axes = plt.subplots(1, 2, figsize=(9.5, 4))\n","        \n","        self.fig.suptitle(title)\n","        self.fig.subplots_adjust(wspace=0.3)\n","        \n","    def update(self, X, labels, cluster_centers, itr, sse):        \n","        # Get left and right axes\n","        ax_l, ax_r = self.axes\n","        ax_l.clear()\n","        ax_r.clear()\n","        \n","        # Plot clusters in left figure using our orignal plot_data function\n","        plot_data(ax_l, X, labels, cluster_centers)\n","        \n","        # Store current iteration and sse\n","        self.itrs.append(itr)\n","        self.sses.append(sse)\n","        \n","        # Plot SSE in right figure\n","        if len(self.itrs) > 1:\n","            ax_r.plot(self.itrs, self.sses)\n","\n","            # Set Labels and Limits\n","            ax_r.set_xlabel('Iteration')\n","            ax_r.set_ylabel('Sum of Squared Error (SSE)')\n","            ax_r.set_xlim(min(self.itrs), max(self.itrs))\n","            ax_r.set_ylim(min(self.sses) - 2, max(self.sses) + 2)\n","                        \n","        # Redraw plot\n","        self.fig.canvas.draw()\n","        self.fig.canvas.flush_events()\n","\n","        # Sleep for a little while\n","        time.sleep(self.sleep_time)           "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e706127cf050fedbf677aa2c865ffcfc","grade":false,"grade_id":"cell-54285aec2c824885","locked":false,"schema_version":3,"solution":true,"task":false},"id":"x9dCledc2ZNe","colab_type":"code","colab":{}},"source":["@contract(k='int, >0',\n","          X='array[NxM], N>0, M>0',\n","          max_itr='int, >0',\n","          threshold='float, >0')\n","def k_means(k, X, max_itr=1000, threshold=1e-2, plot=None):\n","    \"\"\"Naive k-Means algorithm\n","    \n","    You have to implement the k-Means algorithm using the functions\n","    defined above. Remember that the k-Means algorithm consists of\n","    the following steps:\n","     \n","      1. Initialize cluster centers\n","      2. Assign each datapoint in X to a cluster\n","      3. Update the cluster centers\n","      4. Compute the SSE (distortion)\n","      5. Stop if the algorithm has converged or reached the \n","         maximum number of iterations. Otherwise go to Step 2.\n","    \n","    \n","    Some additional notes:\n","    \n","      - A helper class for visualizing the progress of the k-Means\n","        algorithm is provided and passed as the `plot` argument into\n","        this function. After step 4 you can update the figure using\n","        the following lines:\n","        \n","          # Plot iteration results\n","          if plot is not None:\n","              plot.update(X, labels, centers, itr, sse)\n","              \n","        where `X` is the dataset, `labels`are labels assigned to each datapoint\n","        in X based on the current cluster centers, `centers` are the \n","        current cluster centers, `itr` is the current iteration and \n","        `sse` is the current Sum of Squared Errors.\n","        \n","      - You can create a loop that goes from `0` to `max_itr` and use\n","        a `break` statement when the convergence criteria is met, i.e.,\n","        the `is_converged` function returns `True`.\n","        \n","      - When the algorithm stops at step 5, the current value of labels was\n","        determined using the cluster centers of the previous iteration. Re-assign\n","        the cluster labels using the latest values for the cluster centers.\n","    \n","    \n","    :param k: Number of clusters (int)\n","    :param X: Dataset (numpy array)\n","    :param max_itr: Maximum number of iterations (int)\n","    :param threshold: Stopping threshold for change in SSE (float)\n","    :param plot: PlotKMeans instance for visualizing results or None \n","                 if you don't want to visualize the progress (PlotKMeans or None)\n","    :returns: A Python tuple containing:\n","                :param centers: Cluster centers\n","                :param labels: The labels for each datapoint in X calculated with the latest cluster\n","                               centers\n","                :param itr: Final iteration\n","                :param sse: Final SSE\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return centers, labels, sse, itr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"828675bfffb01e22a8fcbb3c9ec6ae24","grade":true,"grade_id":"cell-1192bf3626ab8a73","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"8MxMkHSp2ZNg","colab_type":"code","colab":{}},"source":["k = 3\n","plot = PlotKMeans(title=\"{} clusters\".format(k), sleep_time=0.25)\n","centers, labels, sse, itr = k_means(k, blobs, max_itr=100, plot=plot)\n","\n","print(\"\\nAlgorithm converged in {} iterations!\\n\".format(itr))\n","print(\"Final SSE: {}\".format(sse))\n","print(\"Cluster means:\")\n","for label in range(k):\n","    print(\"  Cluster {}: {}\".format(label, centers[label, :]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ebb08146765074cd2a4734a3298928e6","grade":false,"grade_id":"cell-bdb5e93a7fc75fa2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"LUBPD-VM2ZNh","colab_type":"text"},"source":["### Task 1.8: Choosing number of clusters using SSE\n","Usually you do not know beforehand what the ideal number of clusters is for you dataset. One way to determine the number of clusters is by comparing the final SSE for different number of clusters."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6061da22ecb2d4f8300c821333936740","grade":false,"grade_id":"cell-33c4e20c4cc7250a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"eRUbGStf2ZNi","colab_type":"code","colab":{}},"source":["range_k = range(1, 20)\n","sses = []\n","\n","for k in range_k:\n","    _, _, sse, itr = k_means(k, blobs, max_itr=100, plot=None)\n","    print(\"n_clusters = {}, converged in {} iterations, sse = {:.2f}\".format(k, itr, sse))\n","    sses.append(sse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a9a78a7355609942124b6f113f469e82","grade":false,"grade_id":"cell-79d5958d28f57fe0","locked":true,"schema_version":3,"solution":false,"task":false},"id":"LMhDVrh32ZNk","colab_type":"code","colab":{}},"source":["fig = plt.figure()\n","ax = fig.gca()\n","ax.plot(range_k, sses)\n","ax.set_xlabel(\"Number of clusters\")\n","ax.set_ylabel(\"Sum Squared Errors\")\n","ax.set_xlim(range_k[0], range_k[-1])\n","_ = ax.set_ylim(0, max(sses))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d8e78a5e9a590a2bb3ac60069e2f35f4","grade":false,"grade_id":"cell-dee675bdb66f3d37","locked":true,"schema_version":3,"solution":false,"task":false},"id":"f50sDYBu2ZNl","colab_type":"text"},"source":["As you can see, the SSE declines in an exponential way for a greater number of clusters. It is important to note that the SSE will keep decreasing and will eventually go to zero when the number of clusters equals the number of datapoints.\n","\n","Think about why that is and when looking at the Figure what do you think would be a good number of clusters? Give your answer in the cell below."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"5eb54f616922d8a282e46ce410cbddc7","grade":false,"grade_id":"cell-45f1735b296730bc","locked":false,"schema_version":3,"solution":true,"task":false},"id":"BG8zuCDo2ZNm","colab_type":"code","colab":{}},"source":["# Assign the number of clusters to variable k:\n","#   k =\n","#\n","# The next cell will use this variable to solve the k-means problem\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"3a8d78e24b1e9b74a7253631eab71a80","grade":true,"grade_id":"cell-7f2a01cda7170e75","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"UPwm-FSt2ZNp","colab_type":"code","colab":{}},"source":["plot = PlotKMeans(title=\"{} clusters\".format(k), sleep_time=0.25)\n","centers, labels, sse, itr = k_means(k, blobs, max_itr=100, plot=plot)\n","\n","print(\"\\nAlgorithm converged in {} iterations!\\n\".format(itr))\n","print(\"Final SSE: {}\".format(sse))\n","print(\"Cluster means:\")\n","for label in range(k):\n","    print(\"  Cluster {}: {}\".format(label, centers[label, :]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"5af26b8b62e8595c5ea503aaad1e66de","grade":false,"grade_id":"cell-6f90829bc778aed1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Qpw_XA262ZNr","colab_type":"text"},"source":["We know that our blob data set was generated using 4 centers. There is a big change this also the value you choose above, if not run the above cell with `k=4`. \n","\n","Now it is interesting to compare the centers found by our k-means algorithm with the true centers. The cell below prints the true centers. They centers are probably in a different sequence but you can match each true center pretty well with a center found by the k-Means algorithm."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"98823d94e7ab0812ee5226948a805d12","grade":false,"grade_id":"cell-b670f667336e5555","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ZyX6lsWS2ZNr","colab_type":"code","colab":{}},"source":["for center in blobs_centers:\n","    print(center)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a27c81476bb8403c9b9abaa9406e4e78","grade":false,"grade_id":"cell-de51624677bd2812","locked":true,"schema_version":3,"solution":false,"task":false},"id":"cyUQRR3a2ZNt","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","# Part 2: Fuzzy k-Means <a name='fuzzy_k_means' />\n","In the second part of this assignment you will be implementing Fuzzy k-Means. Some of the components of the k-Means algorithm you implemented above can be reused, like the `init_cluster_centers` function."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e18ae58d608780a68351b9a9def662cd","grade":false,"grade_id":"cell-f0b80d63ebd434bc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"EXdQUAFp2ZNt","colab_type":"text"},"source":["### Task 2.1: Computing the weight matrix\n","In Fuzzy k-Means a single datapoint can belong to multiple clusters. The degree of membership of a datapoint $\\mathbf{x}_i$ to cluster $k$ is given by the probability $P(w_k | \\mathbf{x}_i)$. The probability is calculated using the following equation:\n","\n","$$P(w_k | \\mathbf{x}_i) = \\frac{1}{\\sum_{j=0}^K\\left(\\frac{\\|\\mathbf{x}_i - \\mathbf{y}_k\\|}{\\|\\mathbf{x}_i - \\mathbf{y}_j\\|}\\right)^{\\frac{2}{m - 1}}},$$\n","\n","where $K$ is the total number of clusters, $y_i$ is the center of cluster $i$ and $m$ is the blending parameter (See slide 25 of the VO for more information).\n","\n","The first task is to compute the weight matrix:\n","\n","$$w = \\left[\\begin{array}{cccc}\n","    P(w_0 | \\mathbf{x}_0) & P(w_1 | \\mathbf{x}_0) & \\cdots & P(w_K | \\mathbf{x}_0) \\\\\n","    P(w_0 | \\mathbf{x}_1) & P(w_1 | \\mathbf{x}_1) & \\cdots & P(w_K | \\mathbf{x}_1) \\\\\n","    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n","    P(w_0 | \\mathbf{x}_N) & P(w_1 | \\mathbf{x}_N) & \\cdots & P(w_K | \\mathbf{x}_N) \\\\\n","\\end{array}\\right],$$\n","\n","where $K$ is the number of clusters and $N$ the number of datapoints in our dataset."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"4d09e11bad0d8c1bf125cc5bf0777da0","grade":false,"grade_id":"cell-56946aff2224bd01","locked":false,"schema_version":3,"solution":true,"task":false},"id":"HLl0wqrt2ZNu","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          centers='array[KxM], K>0, M>0',\n","          m='float | int, > 1')\n","def compute_weight_matrix(X, centers, m):\n","    \"\"\"Compute the weight matrix w as defined above\n","    \n","    Note: keep in mind that division by zero can occur.\n","          Implement appropriate measure to prevent this\n","          from happening.\n","          \n","    Note: the weight matrix w constists of conditional \n","          probabilities, this means that the sum of each\n","          row has to equal one. (Why equals the sum of each\n","          row one and not each column?)\n","          \n","    :param X: Dataset (numpy array)\n","    :param centers: Cluster centers (numpy array)\n","    :param m: Blending parameters (float)\n","    :returns: Weight matrix w (numpy array)\n","    \"\"\"\n","    assert m > 1\n","    \n","        \n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"da3bb1439879dd3a55d66216faac5751","grade":true,"grade_id":"cell-72f03faa81ed5bf2","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"69jZoriI2ZNw","colab_type":"code","colab":{}},"source":["# Test on some random data\n","m = 2\n","X = np.random.uniform(-5., 5., size=(10, 2))\n","centers = np.array([[1, 2], [-1, 2], [0.4, 4]])\n","\n","w = compute_weight_matrix(X, centers, m)\n","\n","assert w.shape == (X.shape[0], centers.shape[0])\n","assert np.allclose(np.sum(w, axis=1), 1.)\n","\n","# Test on blobs datadataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","\n","w = compute_weight_matrix(blobs, centers, m)\n","\n","assert w.shape == (blobs.shape[0], k)\n","assert np.allclose(np.sum(w, axis=1), 1.)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1789ac6a474eacb0ee89d980ee1e4084","grade":false,"grade_id":"cell-c93a410b2b798711","locked":true,"schema_version":3,"solution":false,"task":false},"id":"_XhiuDaI2ZNy","colab_type":"text"},"source":["### Task 2.2: Compute cluster centers\n","Compute the cluster centers using the weight matrix according to:\n","\n","$$\\mathbf{y}_k = \\frac{\\sum_{i=0}^{N}P(w_k | \\mathbf{x}_i)^m\\mathbf{x}_i}{\\sum_{i=0}^NP(w_k | \\mathbf{x}_i)^m}$$"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"8d98bfe7ed75242e25c7f088a3c009a5","grade":false,"grade_id":"cell-447731aee97bcf9c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wsExe59_2ZNy","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          w='array[NxK], N>0, K>0',\n","          m='float | int, >1')\n","def compute_centers_fuzzy(X, w, m):\n","    \"\"\"Compute centers for fuzzy k-means\n","    \n","    :param X: Dataset (numpy array)\n","    :param w: Weight matrix (numpy array)\n","    :param m: Blending paramter (float)\n","    :returns: Cluster centers (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    return centers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"878e8cf7606f8b7b904f7c3e7e72e689","grade":true,"grade_id":"cell-d7f50cd94ac4d3d3","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"G7kCSz3V2ZN0","colab_type":"code","colab":{}},"source":["# Test on some random data\n","m = 2\n","X = np.random.uniform(-5., 5., size=(10, 2))\n","centers = np.array([[1, 2], [-1, 2], [0.4, 4]])\n","w = compute_weight_matrix(X, centers, m)\n","\n","new_centers = compute_centers_fuzzy(X, w, m)\n","\n","assert new_centers.shape == centers.shape\n","\n","# Test on blobs datadataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","w = compute_weight_matrix(blobs, centers, m)\n","\n","new_centers = compute_centers_fuzzy(blobs, w, m)\n","\n","assert new_centers.shape == centers.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9df1165a23f93a41aa5b30d9ff2c6030","grade":false,"grade_id":"cell-8804ad99a61c7d33","locked":true,"schema_version":3,"solution":false,"task":false},"id":"v_a9B4WU2ZN2","colab_type":"text"},"source":["### Task 2.3: Determine if the algorithm has converged\n","In naive k-Means we compared the SSE between two iteration to determine convergence. For fuzzy k-Means we compare the maximum weight value between two iterations:\n","\n","$$|\\underset{i, k}{\\max}P^{t}(w_k | \\mathbf{x}_i) - \\underset{i, k}{\\max}P^{t+1}(w_k | \\mathbf{x}_i)| < \\epsilon,$$\n","\n","where $|\\cdot|$ denotes the absolute value, superscript $t$ and $t+1$ denote subsequent iterations and $\\epsilon$ some threshold value."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9d69da5e70d61dedb9ed160bbcd44024","grade":false,"grade_id":"cell-1669004faaf568d9","locked":false,"schema_version":3,"solution":true,"task":false},"id":"UIyh1WL42ZN2","colab_type":"code","colab":{}},"source":["@contract(prev_w='array[NxK], N>0, K>0',\n","          new_w='array[NxK], N>0, K>0',\n","          threshold='float, > 0')\n","def is_converged_fuzzy(prev_w, new_w, threshold):\n","    \"\"\"Determine whether fuzzy k-Means has converged\n","    \n","    :param prev_w: Weight matrix of previous iteration (numpy array)\n","    :param new_w: Weight matrix of current iteration (numpy array)\n","    :param threshold: Threshold for comparing maximum weight values (float)\n","    :param returns: True if converged, False otherwise (bool)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return converged"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7a796939f21b62659696c93144732446","grade":true,"grade_id":"cell-f509c46924652de2","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"yJIJG3Rn2ZN4","colab_type":"code","colab":{}},"source":["# Test on some random data\n","threshold = 1e-5\n","m = 2\n","k = 3\n","X = np.random.uniform(-5., 5., size=(10, 2))\n","centers = init_cluster_centers(X, k)\n","w = compute_weight_matrix(X, centers, m)\n","centers = compute_centers_fuzzy(X, w, m)\n","new_w = compute_weight_matrix(X, centers, m)\n","\n","assert is_converged_fuzzy(w, new_w, threshold) == False\n","assert is_converged_fuzzy(new_w, new_w, threshold) == True\n","\n","\n","# Test on blobs dataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","w = compute_weight_matrix(blobs, centers, m)\n","centers = compute_centers_fuzzy(blobs, w, m)\n","new_w = compute_weight_matrix(blobs, centers, m)\n","\n","assert is_converged_fuzzy(w, new_w, threshold) == False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2698f363692fa53b43006c1ae18ad8af","grade":false,"grade_id":"cell-91b76993ca92e42c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8yn0dYVc2ZN6","colab_type":"text"},"source":["### Task 2.4: Assigning cluster label\n","In the end we often still want to assign a datapoint to a single cluster. In fuzzy k-Means we assign a datapoint to the cluster for which the membership probability is the highest:\n","\n","$$\\text{label}_i = \\underset{k}{\\arg \\max}P(w_k | \\mathbf{x_i}),$$\n","\n","where $\\text{label}_i$ is the label for datapoint $\\mathbf{x}_i$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2d8c195432e95239c4fa0cede58e36b1","grade":false,"grade_id":"cell-faa91ccdd3353741","locked":false,"schema_version":3,"solution":true,"task":false},"id":"yEIe25zz2ZN6","colab_type":"code","colab":{}},"source":["@contract(w='array[NxK], N>0, K>0',\n","          returns='array[N], N>0')\n","def assign_datapoints_to_cluster_fuzzy(w):\n","    \"\"\"Assign cluster labels to each datapoint\n","    \n","    Note: notice that we do not need to use dataset\n","          X because each row in w represents a datapoint\n","          in our dataset X.\n","    \n","    :param w: Weight matrix (numpy array)\n","    :returns: Labels of assigned cluster (numpy array)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"26696198d14fe560e702c2b574f1d209","grade":true,"grade_id":"cell-4dcabb687346291d","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"eJXkduC92ZN8","colab_type":"code","colab":{}},"source":["# Test on some random data\n","threshold = 1e-5\n","m = 2\n","k = 3\n","X = np.random.uniform(-5., 5., size=(10, 2))\n","centers = init_cluster_centers(X, k)\n","w = compute_weight_matrix(X, centers, m)\n","\n","labels = assign_datapoints_to_cluster_fuzzy(w)\n","\n","assert labels.shape == (X.shape[0],)\n","\n","# Test on blobs dataset\n","k = 3\n","centers = init_cluster_centers(blobs, k)\n","w = compute_weight_matrix(blobs, centers, m)\n","\n","labels = assign_datapoints_to_cluster_fuzzy(w)\n","\n","assert labels.shape == (blobs.shape[0],)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"eabc1051216cdc1f40b8894f3a1619d1","grade":false,"grade_id":"cell-2da30e82d39dbf44","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ftcmIA482ZOB","colab_type":"text"},"source":["### Task 2.5: Combining all the parts\n","We can now put all the parts together. The sum of squared errors (SSE) was the objective function used by the naive k-means algorithm and therefore had an integral part in the algorithm. But it is also used as metric for evaluating the quality of a trained clustering algorithm. We will use it again for evaluating and tracking the progress of our fuzzy k-Means clustering algorithm.\n","\n","In your implementation of fuzzy k-Means you can use the previous implemention `compute_sse` together with the `assign_datapoint_to_cluster_fuzzy` function for calculating the SSE."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"da2ba42955580bb05c10a5dc1485f00c","grade":false,"grade_id":"cell-c4a40fd84d2b8608","locked":false,"schema_version":3,"solution":true,"task":false},"id":"4s4bVTqX2ZOB","colab_type":"code","colab":{}},"source":["@contract(k='int, >0',\n","          m='float | int, >1',\n","          X='array[NxM], N>0, M>0',\n","          max_itr='int, >0',\n","          threshold='float, >0')\n","def fuzzy_k_means(k, m, X, max_itr=1000, threshold=1e-5, plot=None):\n","    \"\"\"Fuzzy k-Means algorithm\n","    \n","    You have to implement the fuzzy k-Means algorithm using the functions\n","    defined above and some functions from the naive k-Means algorithm. \n","    \n","    The fuzzy k-Means algorithm consists of the following steps:\n","     \n","      1. Initialize cluster centers (use `init_cluster_centers` function)\n","         and initialize weight matrix w\n","      2. Update cluster centers\n","      3. Update weight matrix w\n","      4. Stop if the algorithm has converged or reached the \n","         maximum number of iterations. Otherwise go to Step 2.\n","       \n","       \n","    Additional steps to implement:\n","      \n","      - Use the original `compute_sse` function to compute the sum of\n","        squared errors (SSE) in each iteration. In order to use this \n","        function you need to compute the cluster labels first.\n","    \n","    \n","    Some additional notes:\n","    \n","      - A helper class for visualizing the progress of the k-Means\n","        algorithm is provided and passed as the `plot` argument into\n","        this function. After step 4 you can update the figure using\n","        the following lines:\n","        \n","          # Plot iteration results\n","          if plot is not None:\n","              plot.update(X, labels, centers, itr, sse)\n","              \n","        where `X` is the dataset, `labels`are labels assigned to each datapoint\n","        in X based on the current cluster centers, `centers` are the \n","        current cluster centers, `itr` is the current iteration and \n","        `sse` is the current Sum of Squared Errors.\n","        \n","      - You can create a loop that goes from `0` to `max_itr` and use\n","        a `break` statement when the convergence criteria is met, i.e.,\n","        the `is_converged` function returns `True`.\n","    \n","    \n","    :param k: Number of clusters (int)\n","    :param X: Dataset (numpy array)\n","    :param m: Blending parameter (float)\n","    :param max_itr: Maximum number of iterations (int)\n","    :param threshold: Stopping threshold for change in SSE (float)\n","    :param plot: PlotKMeans instance for visualizing results or None \n","                 if you don't want to visualize the progress (PlotKMeans or None)\n","    :returns: A Python tuple containing:\n","                :param centers: Cluster centers\n","                :param labels: The labels for each datapoint in X calculated with the latest cluster\n","                               centers\n","                :param itr: Final iteration\n","                :param sse: Final SSE\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return centers, w, sse, itr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7e83da1d022a10a776f9c9062959ca85","grade":true,"grade_id":"cell-5c2c79788e7b6479","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"hWC8KvlB2ZOD","colab_type":"code","colab":{}},"source":["k = 3\n","m = 2\n","plot = PlotKMeans(title=\"{} clusters\".format(k), sleep_time=0.25)\n","centers, w, sse, itr = fuzzy_k_means(k, m, blobs, max_itr=100, plot=plot)\n","\n","print(\"\\nAlgorithm converged in {} iterations!\\n\".format(itr))\n","print(\"Final SSE: {}\".format(sse))\n","print(\"Cluster means:\")\n","for label in range(k):\n","    print(\"  Cluster {}: {}\".format(label, centers[label, :]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"822c4c7a3bfd43e68bfb9f3d28d06327","grade":false,"grade_id":"cell-5196eefb68931788","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8Q0RaMpb2ZOF","colab_type":"text"},"source":["### Task 2.6: Influence of $k$ and $m$\n","The fuzzy k-means algorithm has an extra parameter $m$, called the blending parameter. The question is, how do we choose $m$? For the naive k-means algorithm we compared the SSE for different number of clusters to determine the ideal number of clusters. \n","\n","We can take a similar approach for determining $m$, we will calculate the final SSE for different combinations of $m$ and the number of clusters. The code below will has a double looping over different values of $m$ and number of clusters $k$. It may take a few minutes to compute all the SSE values."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fb4449cd7af402d9bd7dbee858b41755","grade":false,"grade_id":"cell-fb49be109ba38566","locked":true,"schema_version":3,"solution":false,"task":false},"id":"mF663g8r2ZOF","colab_type":"code","colab":{}},"source":["k_range = range(2, 9, 2)\n","m_range = [1.1, 1.5, 2., 5., 10.]\n","sses = []\n","for n_clusters in k_range:\n","    sse_cs = []\n","    for m in m_range:\n","        _, _, sse, itr = fuzzy_k_means(n_clusters, m, blobs, max_itr=50)\n","        print(\"n_clusters = {}, m = {}, converged in {} iterations, sse = {:.2f}\".format(n_clusters, m, itr, sse))\n","        sse_cs.append(sse)\n","    sses.append(sse_cs)\n","sses = np.asarray(sses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f65ccd8479be7f7d979b5074137a685d","grade":false,"grade_id":"cell-e9042c6632aff883","locked":true,"schema_version":3,"solution":false,"task":false},"id":"yAu6wGnM2ZOH","colab_type":"text"},"source":["The next cell will again plot the number of clusters versus the SSE, each line represents a value of $m$"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d9e7a29edbf45838a4b11bdf681caeb6","grade":false,"grade_id":"cell-3bff0c977508067c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"tKIzwbqD2ZOH","colab_type":"code","colab":{}},"source":["fig = plt.figure()\n","ax = fig.gca()\n","ax.plot(k_range, sses)\n","ax.set_xlabel(\"Number of clusters\")\n","ax.set_ylabel(\"Sum Squared Errors\")\n","ax.set_xlim(k_range[0], k_range[-1])\n","ax.set_ylim(0, np.max(sses))\n","ax.legend([r\"m = {}\".format(m) for m in m_range])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"254eb4482e9f7ecb88f7c45a4438d0e3","grade":false,"grade_id":"cell-4bb2e8856f96fd78","locked":true,"schema_version":3,"solution":false,"task":false},"id":"25YBvdif2ZOI","colab_type":"text"},"source":["### Question\n","Observing the figure above, which value of for $m$ do you think is optimal. Choose from the evaluated set $\\{1.1, 1.5, 2.0, 5.0, 10.0\\}$. "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a89cd24166377f8673b4c014fb54f4a5","grade":false,"grade_id":"cell-7fda04d27a580935","locked":false,"schema_version":3,"solution":true,"task":false},"id":"iT4UoIYJ2ZOJ","colab_type":"code","colab":{}},"source":["# Assign to variable m_opt:\n","#   m_opt = \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"391cfef2b654c2b7b2361d7c3ebfbd85","grade":true,"grade_id":"cell-3ddcfbf2a8dc0958","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"kdyoE6cR2ZOL","colab_type":"code","colab":{}},"source":["# This cell contains hidden tests which are not visible for Students. \n","# You don't have to do anything with it.\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cbdb55da885820a7e4b00357e8c469ae","grade":false,"grade_id":"cell-3bcf5f9c9ac832cb","locked":true,"schema_version":3,"solution":false,"task":false},"id":"dFSigmGb2ZON","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","# Part 3: Agglomerative Clustering <a name='agglomerative_clustering'>\n","In the final part we will have a look at agglomerative clustering. Agglomerative is a hierarchical clustering approach. Meaning it creates a hierarchical structure in the data, this means the algorithm allows you to extract anywhere from $k=1$ to $k=N$ clusters from the same representation.\n","\n","The agglomerative algorithm consists of the following 4 steps:\n","\n","1. Start with $N$ singleton clusters\n","2. Find nearest clusters\n","3. Merge them\n","4. If $K > 1$, go to step 2, otherwise stop\n","\n","We have already implemented most of the agglomerative algorithm for you, given in de cells below. Your task will be to implement two linkage functions (*single* and *complete* linkage). The linkage function determines the distance between to clusters. \n","\n","**Single linkage** calculates the smallest distance between two datapoints of two clusters:\n","\n","$$\\underset{\\mathbf{x} \\in \\mathbf{X}_{k=i}, \\mathbf{x}' \\in \\mathbf{X}_{k=j}}{\\min}\\| \\mathbf{x} - \\mathbf{x}' \\|,$$\n","\n","where $\\mathbf{X}_{k=i}$ are datapoints in cluster $k=i$, $\\mathbf{X}_{k=j}$ are datapoints in cluster $k=j$ and $\\|\\cdot\\|$ is the euclidean distance.\n","\n","**Complete linkage** calculates the largest distance between two datapoints of two clusters:\n","\n","$$\\underset{\\mathbf{x} \\in \\mathbf{X}_{k=i}, \\mathbf{x}' \\in \\mathbf{X}_{k=j}}{\\max}\\| \\mathbf{x} - \\mathbf{x}' \\|,$$\n","\n","where $\\mathbf{X}_{k=i}$ are datapoints in cluster $k=i$, $\\mathbf{X}_{k=j}$ are datapoints in cluster $k=j$ and $\\|\\cdot\\|$ is the euclidean distance."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"05b247a132bde3636a55c3f7835aed6e","grade":false,"grade_id":"cell-b472fe1ab40fda2d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NJx0hlIM2ZON","colab_type":"code","colab":{}},"source":["@contract(X='array[NxM], N>0, M>0',\n","          returns='array[N-1x4], N>1')\n","def agglomerative_clustering(X, linkage_fn):\n","    \"\"\"Agglomerative clustering algorithm\n","    \n","    We use the scipy convention for storing the hierarchical\n","    clustering results in a linkage matrix Z. This allows us\n","    to use the dendogram plotting function from scipy for \n","    visualizing the results. For more information on the \n","    linkage matrix see:\n","    \n","      https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n","   \n","    :param X: Dataset (numpy array)\n","    :param linkage_fn: Function returning distance between two clusters (function)\n","    :returns: Linkage matrix Z\n","    \"\"\"\n","    # Initialize empty linkage matrix\n","    Z = []\n","    \n","    # Step 1: Keep track of cluster indices\n","    clusters_idx = [idx for idx in range(X.shape[0])]\n","    \n","    # Keep track of datapoints within each cluster\n","    clusters_points = [[x] for x in X]\n","       \n","    # Cluster index tracker\n","    next_idx = len(clusters_idx)\n","    \n","    # Step 4: Loop until only one cluster is left\n","    while len(clusters_idx) > 1:\n","        # Step 2: Find the two cluster to merge and their distance\n","        i, j, dist = linkage(clusters_points, linkage_fn)\n","        \n","        # Step 3: Merge the original clusters into a new clusters\n","        new_cluster_points = clusters_points[i] + clusters_points[j]\n","                \n","        # Store new cluster index and points in the new cluster\n","        clusters_idx.append(next_idx)\n","        clusters_points.append(new_cluster_points)\n","        \n","        # Store merge in linkage matrix\n","        Z.append([clusters_idx[i], clusters_idx[j], dist, len(new_cluster_points)])\n","        \n","        # Delete merged clusters from clusters_idx and clusters_points\n","        # After merging clusters we cannot merge them again, therefore\n","        # we remove them from the list. The information about this merging\n","        # is stored in linkage matrix Z\n","        for idx in sorted([i, j], reverse=True):\n","            del clusters_idx[idx]\n","            del clusters_points[idx]\n","            \n","        # Increment cluster index\n","        next_idx += 1\n","        \n","    return np.asarray(Z)\n","\n","def linkage(clusters, linkage_fn):\n","    \"\"\"Find the two clusters that are closest to eachother\n","    \n","    The distance between two clusters is determined by the linkage function.\n","    \n","    \"\"\"\n","    n_clusters = len(clusters)\n","    min_dist = np.inf\n","    idx = (-1, -1)\n","    \n","    for i in range(n_clusters):\n","        for j in range(i+1, n_clusters):\n","            dist = linkage_fn(np.asarray(clusters[i]), np.asarray(clusters[j]))\n","            if dist < min_dist:\n","                idx = (i, j)\n","                min_dist = dist\n","            \n","    return idx[0], idx[1], min_dist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"940bc4151eeca1cf39dc31fa5791b733","grade":false,"grade_id":"cell-00eea4f9f7fb18dd","locked":true,"schema_version":3,"solution":false,"task":false},"id":"FPzIfX8I2ZOR","colab_type":"text"},"source":["### Task 3.1: Linkage functions\n","Define a function that computes the single linkage distance and one that computes the complete linkage between two clusters."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1818d37f0438a020ee1ef182f81b4c8d","grade":false,"grade_id":"cell-a3a6fe6ac82ae95b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"of5gSohP2ZOR","colab_type":"code","colab":{}},"source":["@contract(cluster_1='array[AxM], A>0, M>0',\n","          cluster_2='array[BxM], B>0, M>0')\n","def single(cluster_1, cluster_2):\n","    \"\"\"Compute single linkage distance between cluster_1 and cluster_2\n","    \n","    :param cluster_1: (numpy array)\n","    :param cluster_2: (numpy array)\n","    :returns: Minimum distance between cluster_1 and cluster_2\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return dist\n","\n","@contract(cluster_1='array[AxM], A>0, M>0',\n","          cluster_2='array[BxM], B>0, M>0')\n","def complete(cluster_1, cluster_2):\n","    \"\"\"Compute complete linkage distance between cluster_1 and cluster_2\n","    \n","    :param cluster_1: (numpy array)\n","    :param cluster_2: (numpy array)\n","    :returns: Maximum distance between cluster_1 and cluster_2\n","    \"\"\"\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return dist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7c92145f88d8d7e02c946e0d03ad360f","grade":true,"grade_id":"cell-497c9198c2a5c4d1","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"4Y4FfjTa2ZOS","colab_type":"code","colab":{}},"source":["cluster_1 = np.asarray([[1], [8]])\n","cluster_2 = np.asarray([[2], [3], [4]])\n","\n","assert single(cluster_1, cluster_2) < complete(cluster_1, cluster_2)\n","assert np.allclose(single(cluster_1, cluster_2), 1.0)\n","assert np.allclose(single(cluster_2, cluster_1), 1.0) # Ordering of clusters should not matter\n","assert np.allclose(complete(cluster_1, cluster_2), 6.0)\n","assert np.allclose(complete(cluster_2, cluster_1), 6.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"532ed7d93d74e90bf2e44896b2e40580","grade":false,"grade_id":"cell-3e30826cdf1aa0f5","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5SIDS-yu2ZOU","colab_type":"text"},"source":["### Task 3.2: Dendogram\n","Next we will compare the different linkage functions on a simple toy dataset and plot each dendogram. We will use the scipy function `scipy.hierarchy.dendogram` for plotting. See the VO lecture slide 34 and the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) for more information."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"74a02a33049e9cd5c595ce2b0c21cca0","grade":true,"grade_id":"cell-cb3b0ea3fa5c497c","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"kRKFFFF52ZOU","colab_type":"code","colab":{}},"source":["X = np.array([[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]])\n","\n","Z_single = agglomerative_clustering(X, single)\n","Z_complete = agglomerative_clustering(X, complete)\n","\n","fig, axes = plt.subplots(1, 2, sharey=True)\n","hierarchy.dendrogram(Z_single, ax=axes[0])\n","hierarchy.dendrogram(Z_complete, ax=axes[1])\n","\n","axes[0].set_ylabel(\"Distance\")\n","axes[0].set_title(\"Single Linkage\")\n","axes[1].set_title(\"Complete Linkage\")\n","\n","print(\"single linkage:\\n{}\".format(Z_single))\n","print(\"\")\n","print(\"complete linkage:\\n{}\".format(Z_complete))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2675ae633a6ee620766704da726998ab","grade":false,"grade_id":"cell-71a6589aad2e316d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"vNZ3V2Fv2ZOW","colab_type":"text"},"source":["### Scipy implementation\n","Below we perform the same agglomerative clustering using Scipy. The results should be the same as above. Note that the linkage matrices might be different because the merging sequence can be different when more then two clusters have the same distance to each other. The plots should show the same results"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"66ef72d3613905dbf9ff38c2ec3e5260","grade":false,"grade_id":"cell-6ce108a857b25397","locked":true,"schema_version":3,"solution":false,"task":false},"id":"xB9Mqlm52ZOW","colab_type":"code","colab":{}},"source":["X = np.array([[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]])\n","                            \n","Z_single_scipy = hierarchy.linkage(X, 'single')\n","Z_complete_scipy = hierarchy.linkage(X, 'complete')\n","\n","fig, axes = plt.subplots(1, 2, sharey=True)\n","hierarchy.dendrogram(Z_single_scipy, ax=axes[0])\n","hierarchy.dendrogram(Z_complete_scipy, ax=axes[1])\n","\n","axes[0].set_ylabel(\"Distance\")\n","axes[0].set_title(\"Single Linkage\")\n","axes[1].set_title(\"Complete Linkage\")\n","\n","print(\"single linkage:\\n{}\".format(Z_single_scipy))\n","print(\"\")\n","print(\"complete linkage:\\n{}\".format(Z_complete_scipy))"],"execution_count":0,"outputs":[]}]}