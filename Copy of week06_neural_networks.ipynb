{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"week06_neural_networks.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"81230431a2395b32415744a5e0a9a0a2","grade":false,"grade_id":"cell-bc17519ab800ccee","locked":true,"schema_version":3,"solution":false,"task":false},"id":"CgwrFnmleQE3","colab_type":"text"},"source":["# Overview  <a name='objectives' />\n","\n","This notebook introduces you to the basics of Neural Networks. We will start by implementing and training a simple neural network using Numpy. Later, we will move on to training fully connected and convolutional neural networks using the PyTorch library.\n","\n","The topics that will be covered are:\n","\n","1. <a href=#numpy_nn>Binary Classification with a Simple Neural Network (using Numpy)</a>\n","2. <a href=#pytorch_basics>PyTorch Basics</a>\n","3. <a href=#multi>Multiclass Classification with PyTorch</a>\n","4. <a href=#cnn>Convolutional Neural Networks with PyTorch</a>\n","\n","### Programming Tasks\n","For the programming tasks you will need to replace the following comment and exception with your own code:\n","\n","```python\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","```\n","\n","Most programming tasks are followed by a cell with tests (using the `assert` function from python). You can use these cells while developing your implementation and for validating your implementation.\n","\n","**<font size=\"3\" color=\"red\">Note</font>**: The `@contract` decorators make sure the data types and shapes are correct for the inputs and outputs. See [here](https://andreacensi.github.io/contracts/tour.html#quick-tour) for more. If you are more comfortable working without these, you can comment out the lines starting with `@contract`. However, in that case it can get tedious to locate the exact source of a bug.\n","\n","### Open Questions\n","The notebook also contains a few open questions. For the open questions you can put your answer in the cell below the question."]},{"cell_type":"code","metadata":{"id":"nTWdBudde8Ol","colab_type":"code","outputId":"81ed992c-fdac-44e8-9264-84cdc5c3a498","executionInfo":{"status":"ok","timestamp":1587653351703,"user_tz":-120,"elapsed":28694,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements_week06.txt'\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_gTjLzqFg-pN","colab_type":"code","outputId":"c29b29ba-44d0-4447-bed7-e62752e28480","executionInfo":{"status":"error","timestamp":1587653480316,"user_tz":-120,"elapsed":823,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["with open('/content/drive/folders/1RQtzD4COD7OTUdvQS8xR3wgcerMCqZK5/requirements_week06.txt', 'w') as f:"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ef579a94e115>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with open('/content/drive/folders/1RQtzD4COD7OTUdvQS8xR3wgcerMCqZK5/requirements_week06.txt', 'w') as f:\u001b[0m\n\u001b[0m                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"573a6fa456736a78d16fe3ebd50113b4","grade":false,"grade_id":"cell-748d011c2ee07431","locked":true,"schema_version":3,"solution":false,"task":false},"id":"LazGAlyZeQE4","colab_type":"code","outputId":"b442db7e-e902-49c3-8c03-48b08ed5d51f","executionInfo":{"status":"ok","timestamp":1587652701760,"user_tz":-120,"elapsed":4617,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# DO NOT INSTALL THE LIBRARIES WHEN WORKING ON ifi-europa.uibk.ac.at\n","\n","# Make sure that the required libraries are installed\n","# If you are using Google Colab, remember to upload the requirements file before \n","# running this cell\n","# If you are running this notebook locally, the requirements file needs to be in \n","# the same location as this notebook\n","import os\n","running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n","    \n","if running_local:\n","    import sys\n","    !pip install -r requirements_week06.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements_week06.txt'\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2277ce52b08c4e86d5a6bc7f8a959d7c","grade":false,"grade_id":"cell-2357dcfd390a9c7e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"61qefEP6eQE7","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","### Setup"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ec57ef19038a907eb7ce9f982f01c2eb","grade":false,"grade_id":"cell-177b8675e170a66b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"xw2Asxv5eQE8","colab_type":"code","colab":{}},"source":["from tqdm.notebook import tqdm, trange\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from IPython.display import set_matplotlib_formats\n","#set_matplotlib_formats('svg')\n","from contracts import contract\n","import sklearn\n","from sklearn import cluster, datasets\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"85138bd9c05890ac29dba9fd6c8e1b73","grade":false,"grade_id":"cell-3f7792bf33c5ef20","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NE0QM5ZMeQE-","colab_type":"code","colab":{}},"source":["# Random seed for reproducability\n","random_seed = 123\n","np.random.seed(random_seed)\n","\n","# Total number of data points\n","n_samples = 1500\n","\n","# Toy dataset\n","moons = datasets.make_moons(n_samples=n_samples, noise=.35, random_state=random_seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d62acd007b983b002b3f56d9fb9e5a5c","grade":false,"grade_id":"cell-50f1a0a6d859b0e4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"29X_-crLeQFA","colab_type":"text"},"source":["You will be working again with the `moons` dataset from `sklearn`. Let's load the data and split it up into train and test sets. The functions for doing this are provided to you below."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e23e519aefe634599b38c0acd9e588b6","grade":false,"grade_id":"cell-a27d5667136cafc2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"GhsfudHDeQFA","colab_type":"code","colab":{}},"source":["# Function for splitting into train and test sets\n","def split(dataset):\n","    \"\"\"\n","    Splits a dataset from sklearn into train and test sets.\n","    \n","    :param: dataset: sklearn dataset (data, labels) (2-tuple of numpy arrays)\n","    :returns: x_train, x_test, y_train, y_test (4-tuple of numpy arrays)\n","    \"\"\"\n","\n","    # Get data and labels\n","    X,Y = dataset\n","\n","    # Reshape Y to [num_points, 1]\n","    Y = np.expand_dims(Y, axis=1)\n","\n","    # Split the data into train and test sets\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=random_seed)\n","    \n","    print(\"Shape of data:\")\n","    print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}, Y_train: {Y_train.shape}, Y_test: {Y_test.shape}\")\n","    return X_train, X_test, Y_train, Y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9fb5419355c6456d34d4cbd2e3804072","grade":false,"grade_id":"cell-cd970c1b2b096cfa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"zr8BPkr2eQFC","colab_type":"code","colab":{}},"source":["moons_x_train, moons_x_test, moons_y_train, moons_y_test = split(moons)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"87d2c7d9e2b1036989a1d0217d3428f3","grade":false,"grade_id":"cell-540c229051bb071b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"LMjLdpf6eQFE","colab_type":"code","colab":{}},"source":["# Visualize the data\n","\n","plt.figure(figsize=(8,5))\n","\n","markers = ('s', '^', 'x', 'o', 'v')\n","colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n","cmap = ListedColormap(colors[:len(np.unique(moons_y_train))])\n","    \n","for idx, yv in enumerate(np.unique(moons_y_train[:,0])): \n","    plt.scatter(x=moons_x_train[moons_y_train[:,0]==yv, 0], \n","                y=moons_x_train[moons_y_train[:,0]==yv, 1], \n","                alpha=0.6, \n","                c=[cmap(idx)], \n","                marker=markers[0], \n","                label=f\"Train data class {yv}\",\n","                edgecolors='k',\n","                s=20)\n","    \n","for idx, yv in enumerate(np.unique(moons_y_test[:,0])): \n","    plt.scatter(x=moons_x_test[moons_y_test[:,0]==yv, 0], \n","                y=moons_x_test[moons_y_test[:,0]==yv, 1], \n","                alpha=1.0, \n","                c=[cmap(idx)], \n","                marker=markers[1], \n","                label=f\"Test data class {yv}\",\n","                edgecolors='k',\n","                s=20)\n","\n","plt.legend(ncol=2, fontsize=8)\n","plt.title('Moons Dataset')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"27bfae63e7d89cc79464198ae53ef3c6","grade":false,"grade_id":"cell-2076209c2f85d39f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"FS6WowIaeQFH","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","## Part 1: Binary Classification with a Simple Neural Network (using Numpy) <a name='numpy_nn' />"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fda28a457033000b7425cf681687532e","grade":false,"grade_id":"cell-92b6f69f372dca53","locked":true,"schema_version":3,"solution":false,"task":false},"id":"I2AMlGqneQFH","colab_type":"text"},"source":["For this task you will be building and training a simple neural network (consisting of 1 hidden layer) using numpy. A diagram of this simple neural network is provided below, along with the shapes of all the numpy arrays that you will be using. \n","\n","In particular, note that data will be fed into the network in batches. For example, in the figure below, the input $X$ is of shape $[m, 2]$, which means that each data point in $X$ is 2-dimensional and there are $m$ such data points (batch size is $m$). $X$ is then combined with the weights $W_1$ and biases $b_1$ to produce the linear output $Z_1$. $Z_1$ is then passed through $g_1$, the activation function of the hidden layer to produce the hidden layer output $A_1$. $A_1$ now forms the input to the output layer, and is combined with $W_2$ and $b_2$ to produce $Z_2$, which is fed in to $g_2$, the activation function of the hidden layer to produce $A_2$, which is the network's prediction for $X$. Since this network will be used for binary classification, $g_2$ is the sigmoid function and $A_2$ has the shape $[m, 1]$."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1a004461a74d6a9c8bb955c20cac9bbe","grade":false,"grade_id":"cell-cac1ce00b4f4a340","locked":true,"schema_version":3,"solution":false,"task":false},"id":"XGEC579heQFH","colab_type":"text"},"source":["<img src=\"https://iis.uibk.ac.at/public/auddy/images/small_net.svg\" alt=\"Image of Neural Network\" width=\"700\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2beb0331d4efc6445c4e1470fb0b08d9","grade":false,"grade_id":"cell-a0e41a7d7d224c76","locked":true,"schema_version":3,"solution":false,"task":false},"id":"050NmnBseQFI","colab_type":"text"},"source":["For building and training the network, the following steps need to be performed:\n","#### Initialize $\\rightarrow$ Forward $\\rightarrow$ Calculate Loss $\\rightarrow$ Backward $\\rightarrow$ Weight Update"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9401f8bb8efe0d41b2f6a34793511e13","grade":false,"grade_id":"cell-48beffc849ead914","locked":true,"schema_version":3,"solution":false,"task":false},"id":"slxmAh0WeQFI","colab_type":"text"},"source":["### Task 1.1: Numpy Network: Initialization\n","\n","int: (valid_name, int))The first step of building the network shown above is to initialize the weights and biases of the network. For the weights, you need to construct numpy arrays which are initialized randomly. But these random weights should be close to zero (but not all zero). You can use the `np.random.randn` function with the correct shape for this, but `np.random.randn` returns numbers in the range $[-1.0, 1.0]$. If you use exactly the returned values, there is a high chance of numerical overflow while training the network. You will have to modify the returned values so that they are much smaller (e.g. divide by a factor of $100$). For the biases, you can use `np.zeros` with the proper shape. Complete the missing parts of the function `init_model` for initializing the network."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0f1e469e5b076b118125f4085d70d794","grade":false,"grade_id":"cell-f0903677e97ebe4e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"JxtMPBYxeQFJ","colab_type":"code","outputId":"7925af1e-0291-4d87-e077-611df366fd4c","executionInfo":{"status":"error","timestamp":1587666257441,"user_tz":-120,"elapsed":815,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["@contract(X='array[MxD],M>0,D>0', \n","          Y='array[Mx1],M>0', \n","          n_hidden='int,>0')   \n","def init_model(X, Y, n_hidden):\n","    \"\"\"\n","    Function for initializing the parameters (weights and biases) of our neural network.\n","    We are working with a network which has 1 hidden layer and 1 output layer.\n","    \n","    param: X: Input data (numpy array of shape [m,input_dim])\n","    param: Y: Target labels (numpy array of shape [m,output_dim])\n","    param: n_hidden: Number of hidden units (int)\n","\n","    returns: parameters:  python dictionary containing the initialized parameters:\n","                          W1 - numpy array of shape (n_hidden, input_dim)\n","                          b1 - numpy array of shape (1, n_hidden)\n","                          W2 - numpy array of shape (output_dim, n_hidden)\n","                          b2 - numpy array of shape (1, output_dim)\n","    \"\"\"\n","    \n","    # Calculate the number of nodes in the input and output layers\n","    input_dim = X.shape[1] \n","    output_dim = Y.shape[1]\n","\n","    #np.random.seed(1)\n","    \n","    # Initialize the weights and biases\n","    W1 = np.random.randn(n_hidden,input_dim)*0.01\n","    b1 = np.zeros((n_hidden,1))\n","    W2 = np.random.randn(output_dim,n_hidden)*0.01\n","    b2 = np.zeros((output_dim,1))\n","\n","    # assign to the variables W1, b1, W2 and b2\n","    assert(W1.shape == (n_hidden, input_dim))\n","    assert(b1.shape == (n_hidden, 1))\n","    assert(W2.shape == (output_dim, n_hidden))\n","    assert(b2.shape == (output_dim, 1))\n","\n","\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    parameters = {'W1': W1,\n","                  'b1': b1,\n","                  'W2': W2,\n","                  'b2': b2}\n","    \n","    return parameters"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0047d06ce88c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m @contract(X='array[MxD],M>0,D>0', \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'array[Mx1],M>0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           n_hidden='int,>0')   \n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'contract' is not defined"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cca9bf480f99281bdf68fb5303463b22","grade":true,"grade_id":"cell-67a3fa1dca652627","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"IR1P2dE3eQFK","colab_type":"code","colab":{}},"source":["# Test the init function\n","m = 250\n","X = np.random.rand(m, 2)\n","Y = np.random.rand(m, 1)\n","n_hidden = 45\n","parameters = init_model(X, Y, n_hidden)\n","\n","assert parameters[\"W1\"].shape == (n_hidden, 2), \\\n","f\"The shape of W1 is incorrect, it should be ({n_hidden}, 2) but it is {parameters['W1'].shape}\"\n","assert parameters[\"b1\"].shape == (1, n_hidden), \\\n","f\"The shape of b1 is incorrect, it should be (1, {n_hidden}) but it is {parameters['b1'].shape}\"\n","assert parameters[\"W2\"].shape == (1, n_hidden), \\\n","f\"The shape of W2 is incorrect, it should be (1, {n_hidden}) but it is {parameters['W2'].shape}\"\n","assert parameters[\"b2\"].shape == (1, 1), \\\n","f\"The shape of b2 is incorrect, it should be (1, 1) but it is {parameters['b2'].shape}\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ea0f32545387a2e1c286a5c695ae3c70","grade":false,"grade_id":"cell-823e874cc8fc3251","locked":true,"schema_version":3,"solution":false,"task":false},"id":"3kqw4lGpeQFM","colab_type":"text"},"source":["### Task 1.2: Numpy Network: Forward Propagation\n","\n","The steps for forward propagating input data through the network has already been described <a href=#numpy_nn> here</a>. These steps are summarized again below. Complete the functions `sigmoid` for the output activation function. Keep in mind that `sigmoid` should be able to handle both single numbers as well as arrays. Then complete the function `forward_propagation` by implementing these steps:"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9263146c7a769a36c27565dabfabdae7","grade":false,"grade_id":"cell-0bf70fe6400fa1c7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"TgQmBSSAeQFM","colab_type":"text"},"source":["**Steps for Forward Propagation**\n","\n","1. $Z_1 = XW_1^{T} + b_{1}$\n","2. $A_1 = g_{1}(Z_{1})$ (where $g_{1}$ is the activation function of layer 1)\n","3. $Z_2 = A_{1}W_2^{T} + b_{2}$\n","4. $A_2 = g_{2}(Z_{2})$ (where $g_{2}$ is the sigmoid function)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"dc14ee4f93b559673d95309517e7a16f","grade":false,"grade_id":"cell-3d7eada9b2ae9aa2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"dBz4mQnceQFN","colab_type":"text"},"source":["Use `np.tanh` for $g_{1}$ and the sigmoid function for $g_{2}$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2a98ba1689b283979958a1b11ae39946","grade":false,"grade_id":"cell-2729924c7861280f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"khPDGoWFeQFN","colab_type":"code","colab":{}},"source":["@contract(z='array[AxB],A>0,B>0|float',\n","          returns='array[AxB],A>0,B>0|float,>=0.0,<=1.0')\n","def sigmoid(z):\n","    \"\"\"\n","    Computes the sigmoid function. \n","    Capable of vectorizing (works for both single floats as well as numpy arrays)\n","    \n","    param: z: Input (float or numpy array)\n","    returns: Sigmoid of input (float or numpy array)\n","    \"\"\"\n","    sigmoid_value = 1/(1+np.exp(-z))\n","\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return sigmoid_value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"55777ee250893191392eab9e709f5a7f","grade":false,"grade_id":"cell-635a72c0386b7a0d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Lrgn0v_JeQFQ","colab_type":"code","colab":{}},"source":["@contract(X='array[MxD],M>0,D>0')\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Forward-propagates the data through the neural network\n","    \n","    param: X: Input data (numpy array of shape [m,input_dim])\n","    param: python dictionary containing the initialized parameters:\n","                          W1 - numpy array of shape (n_hidden, input_dim)\n","                          b1 - numpy array of shape (1, n_hidden)\n","                          W2 - numpy array of shape (output_dim, n_hidden)\n","                          b2 - numpy array of shape (1, output_dim)\n","    \n","    returns: A2: The network prediction (numpy array of shape [m,output_dim])\n","             cache: python dictionary containing Z1, A1, Z2 and A2\n","    \"\"\"\n","    \n","    # Retrieve weights and biases\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    \n","    \n","    # Implement Forward Propagation to calculate A2 (probabilities)\n","    #   assign to variables Z1, A1, Z2, A2\n","\n","    Z1 = np.dot(X,W1.T) + b1\n","    assert(Z1.shape == (X.shape[0], W1.shape[0])) # looked it up from test block below ??\n","    \n","    #   use `np.tanh` for A1\n","    A1 = np.tanh(Z1)\n","    assert(A1.shape == (X.shape[0], W1.shape[0]))\n","\n","    Z2 = np.dot(A1,W2.T) + b2\n","    assert(Z2.shape == (X.shape[0], 1)\n","   \n","    #   use the `sigmoid` function for A2\n","    A2 = sigmoid(Z2)\n","    assert(A2.shape == (X.shape[0], 1))\n","\n","\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","        \n","    cache = {'Z1': Z1,\n","             'A1': A1,\n","             'Z2': Z2,\n","             'A2': A2}\n","    \n","    return A2, cache  # Think: Why do we return the cache?"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8225263d92e2db05325b46db2e6a719e","grade":true,"grade_id":"cell-01692291dd16c0b5","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"ehye6dZaeQFT","colab_type":"code","colab":{}},"source":["# Test the forward function using the variables from the previous test block\n","A2, cache = forward_propagation(X, parameters)\n","\n","assert A2.shape == (X.shape[0], 1), \\\n","f\"The shape of A2 is incorrect, it should be ({X.shape[0]}, 1) but it is {A2.shape}\"\n","\n","assert cache[\"Z1\"].shape == (m, n_hidden), \\\n","f\"The shape of cache['Z1'] is incorrect, it should be ({m}, {n_hidden}) but it is {cache['Z1'].shape}\"\n","\n","assert cache[\"A1\"].shape == (m, n_hidden), \\\n","f\"The shape of cache['A1'] is incorrect, it should be ({m}, {n_hidden}) but it is {cache['A1'].shape}\"\n","\n","assert cache[\"Z2\"].shape == (m, 1), \\\n","f\"The shape of cache['Z2'] is incorrect, it should be ({m}, 1) but it is {cache['Z2'].shape}\"\n","\n","assert cache[\"A2\"].shape == (m, 1), \\\n","f\"The shape of cache['A2'] is incorrect, it should be ({m}, 1) but it is {cache['A2'].shape}\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"957937943bdd36ca0e8c256d3af65b2a","grade":false,"grade_id":"cell-aebf05d4faddd292","locked":true,"schema_version":3,"solution":false,"task":false},"id":"zFZpOW8WeQFX","colab_type":"text"},"source":["### Task 1.3: Numpy Network: Binary Cross Entropy Loss\n","Complete the function `loss` which computes the binary cross entropy loss $J$, given by the following formula:\n","\n","$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small Y^{(i)}\\log\\left(A_{2}^{(i)}\\right) + (1-Y^{(i)})\\log\\left(1- A_{2}^{(i)}\\right) \\large{)} \\small$$\n","\n","where $m$ is the number of data points, $Y^{(i)}$ is the $i^{\\text{th}}$ target label, and $A_{2}^{(i)}$ is the $i^{\\text{th}}$ prediction.\n","\n","**Hint**: Since $\\log(0) \\rightarrow \\infty$ use of the logarithm might lead to numerical issues (for example for very small numbers). To avoid this issue, add small number `eps` in the log functions $\\log(x + \\mathrm{eps})$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3495afa52fc6a50883aabdef8379026c","grade":false,"grade_id":"cell-4779f7412249d8b1","locked":false,"schema_version":3,"solution":true,"task":false},"id":"LL3hqURfeQFX","colab_type":"code","outputId":"333aa3fe-e020-436d-82aa-82f06b82efc0","executionInfo":{"status":"error","timestamp":1587738399210,"user_tz":-120,"elapsed":742,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["@contract(A2='array[MxO],M>0,O>0',\n","          Y='array[MxO],M>0,O>0',\n","          returns='float, >=0')\n","def loss(A2, Y, params):\n","    \"\"\"\n","    Computes the binary cross entropy loss\n","    \n","    param: A2: The network prediction (numpy array of shape [m,output_dim])\n","    param: Y: Target labels (numpy array of shape [m,output_dim])\n","    param: params: python dictionary containing parameters W1, b1, W2 and b2\n","    \n","    returns: loss: cross-entropy loss (float)\n","    \"\"\"\n","    \n","    eps = 1e-15\n","    \n","    # Number of data points\n","    num_data_points = Y.shape[0]\n","    \n","        \n","    # Compute the cross-entropy loss\n","    loss_value = - (1 / num_data_points) * np.sum(np.multiply(Y, np.log(A2 + eps)) + np.multiply(1-Y, np.log(1 - A2 + eps))) \n","\n","    # multiplication from the right: \n","    # loss_value = - (1 / num_data_points) * np.sum(np.multiply(np.log(A2 + eps),Y) + np.multiply(np.log(1 - A2 + eps),1-Y)) \n","\n","    # don't know if i have to use transpose of A2 all the time to do it right??\n","    # loss_value = - (1 / num_data_points) * np.sum(np.multiply(Y, np.log(A2 + eps).T) + np.multiply(1-Y, np.log(1 - A2 + eps).T))\n","    \n","    # Make sure to return a scalar, maybe this \n","    loss_value = np.squeeze(loss_value)      # To make sure your loss's shape is what we expect (e.g. this turns [[41]] into 41).\n","    \n","    assert(loss_value.shape == ())          # not sure what to take \n","    assert(isinstance(loss_value, float))\n","    \n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return loss_value"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-dfaff01e3df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m @contract(A2='array[MxO],M>0,O>0',\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'array[MxO],M>0,O>0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           returns='float, >=0')\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'contract' is not defined"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5ad1b3a03357eae27a05a78ffd230d9a","grade":true,"grade_id":"cell-3771f2f7198ce843","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"WuDMQrqleQFa","colab_type":"code","colab":{}},"source":["# Test the loss function\n","loss_value = loss(A2, Y, parameters)\n","assert isinstance(loss_value, float) and loss_value>=0.0, \"loss_value should be a float >= 0.0\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3f9254329e4b14d654f2604f9b6d7878","grade":false,"grade_id":"cell-8a307a18c80ed048","locked":true,"schema_version":3,"solution":false,"task":false},"id":"_fR_tCPreQFd","colab_type":"text"},"source":["### Numpy Network: Backward Propagation\n","The backward function is used for computing the gradients of the trainable parameters (the weights and the biases) of the network. The steps for the backward computation are provided below. These formulas were derived assuming that $g_{2}$, the output activation function is sigmoid. All the symbols have the same meaning as in earlier cells. The backward function is provided to you, but you are encouraged to study and understand it."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d084f0e4de04f34640a1b6862c3ddf4f","grade":false,"grade_id":"cell-724be05ba0ece0c6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"l_qBLGEMeQFd","colab_type":"text"},"source":["**Steps for Backward Propagation**\n","\n","where $\\odot$ denotes the Hadamard product.\n","\n","($\\text{d}Z_{2}$ stands for $\\cfrac{\\partial{J}}{\\partial{Z_{2}}}$ and so on)\n","1. $\\text{d}Z_{2} = A_{2} - Y$ (assuming sigmoid function is used in the output layer)\n","2. $\\text{d}W_{2} = \\cfrac{1}{m} \\text{d}Z_{2}^{T} A_{1}$\n","3. $\\text{d}b_{2} = \\cfrac{1}{m}$np.sum($\\text{d}Z_{2}$, axis=0, keepdims=True)\n","4. $\\text{d}Z_{1} = (\\text{d}Z_{2}W_{2})  \\odot (g'_{1}(Z_{1}))$\n","5. $\\text{d}W_{1} = \\cfrac{1}{m} \\text{d}Z_{1}^{T}X$\n","6. $\\text{d}b_{1} = \\cfrac{1}{m}$np.sum($\\text{d}Z_{1}$, axis=0, keepdims=True)"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"549e8f6412c850b680d7bb582167fad7","grade":false,"grade_id":"cell-57cb272283a3ca35","locked":true,"schema_version":3,"solution":false,"task":false},"id":"LWbjnldIeQFe","colab_type":"code","colab":{}},"source":["def backward_propagation(parameters, cache, X, Y):\n","    \"\"\"\n","    Backward function for computing gradients\n","\n","    param: parameters: python dictionary containing parameters W1, b1, W2 and b2\n","    param: cache: python dictionary containing Z1, A1, Z2 and A2\n","    param: X: Input data (numpy array of shape [m,input_dim])\n","    param: Y: Target labels (numpy array of shape [m,output_dim])\n","    \n","    returns: grads: python dictionary containing gradients dW1, db1, dW2 and db2\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    \n","    # First, retrieve W1 and W2 from the dictionary 'parameters'\n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","        \n","    # Retrieve also A1 and A2 from dictionary 'cache'\n","    A1 = cache['A1']\n","    A2 = cache['A2']\n","    \n","    # Backward propagation: calculate dW1, db1, dW2, db2. \n","    dZ2 = A2 - Y\n","    dW2 = np.dot(dZ2.T, A1)/m\n","    db2 = np.sum(dZ2, axis=0, keepdims=True)/m\n","    dZ1 = np.multiply(np.dot(dZ2, W2), (1 - np.power(A1, 2)))\n","    dW1 = np.dot(dZ1.T, X)/m\n","    db1 = np.sum(dZ1, axis=0, keepdims=True)/m\n","    \n","    grads = {'dW1': dW1,\n","             'db1': db1,\n","             'dW2': dW2,\n","             'db2': db2}\n","    \n","    return grads"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"360c286d0bb1e420cc4f1a525453ae89","grade":false,"grade_id":"cell-1999dc10a703db6f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7xXGNyg0eQFg","colab_type":"text"},"source":["### Task 1.4: Numpy Network: Updating Weights\n","After the gradient computation, the weights and biases need to be updated in the direction of the negative gradient by using the learning rate. The weight update is done using the following formula:\n","$$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$$ \n","where $\\alpha$ is the learning rate and $\\theta$ represents a trainable parameter (weight or bias).\n","\n","Complete the function `update_parameters` for performing weight updates."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f60d2700ce11ab430a2eb89220478efd","grade":false,"grade_id":"cell-9b91927e3c5cdafd","locked":false,"schema_version":3,"solution":true,"task":false},"id":"kuRR-YcveQFh","colab_type":"code","colab":{}},"source":["def update_parameters(parameters, grads, learning_rate = 0.01):\n","    \"\"\"\n","    Updates parameters using the gradient descent update rule given above\n","    \n","    param: parameters: python dictionary containing parameters W1, b1, W2 and b2\n","    param: grads: python dictionary containing gradients dW1, db1, dW2 and db2\n","    \n","    returns: parameters: python dictionary containing updated parameters W1, b1, W2 and b2 \n","    \"\"\"\n","    \n","    # Steps:\n","    #   1. Retrieve each parameter (W1, b1, W2, b2) from the dictionary \"parameters\"\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","\n","    #   2. Retrieve each gradient (dW1, db1, dW2, db2) from the dictionary \"grads\"\n","    dW1 = grads['dW1']\n","    db1 = grads['db1']\n","    dw2 = grads['dW2']\n","    db2 = grads['db2']\n","\n","    #   3. Update each parameter (W1, b1, W2, b2) with rule using a for loop. ???\n","    # L = len(parameters) // 2 # number of layers in the neural network\n","    # for l in range(L):\n","    #    parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n","    #    parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+ str(l+1)]\n","\n","    # works better with step by step solution: \n","    W1 = W1 - learning_rate * dW1\n","    b1 = b1 - learning_rate * db1\n","    W2 = W2 - learning_rate * dW2\n","    b2 = b2 - learning_rate * db2\n","\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    parameters = {'W1': W1,\n","                  'b1': b1,\n","                  'W2': W2,\n","                  'b2': b2}\n","    \n","    return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"98e7d58f731a016e39215cf6765e03bc","grade":true,"grade_id":"cell-27584f24dd5946dc","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"uv0-j0oqeQFk","colab_type":"code","colab":{}},"source":["# Test the update function\n","grads = backward_propagation(parameters, cache, X, Y)\n","updated_parameters = update_parameters(parameters, grads, learning_rate = 0.01)\n","\n","for k, v in parameters.items():\n","    assert updated_parameters[k].shape == v.shape, \\\n","    f\"Shape of updated_parameters['{k}']: {updated_parameters[k].shape} is not equal to shape of parameters['{k}']: {v.shape}\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c6f9f96b34971988731f3c3a6dbffd79","grade":false,"grade_id":"cell-cf57d0fd6f550a41","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b-j1DpCceQFm","colab_type":"text"},"source":["### Task 1.5: Numpy Network: Accuracy and Prediction\n","\n","Complete the functions `predict` for making predictions and `calc_accuracy` for computing the accuracy of predictions. The prediction function should assign outputs of the network which are $> 0.5$ to class $1$ and the rest to class $0$."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"30ae1f0eae8b3a1486d3ea5c8f8b675f","grade":false,"grade_id":"cell-14d2ac9096249912","locked":false,"schema_version":3,"solution":true,"task":false},"id":"s0a469jAeQFn","colab_type":"code","colab":{}},"source":["@contract(X='array[MxD],M>0,D>0',\n","          returns='array[Mx1],M>0')\n","def predict(parameters, X):\n","    \"\"\"\n","    Make predictions (class 0 or 1) using the learned parameters\n","    \n","    param: parameters: python dictionary containing parameters W1, b1, W2 and b2\n","    param: X: Input data for which label is to be predicted (numpy array of shape [m,input_dim])\n","        \n","    returns: predictions: Predictions of our model (numpy array of shape [m,1] containing 0s and 1s)\n","    \"\"\"\n","    \n","    # Compute probabilities using forward propagation, and classify to 0/1 using 0.5 as the threshold.\n","    A2, cache = forward_propagation(X,parameters)\n","    predictions = (A2 > 0.5)\n","    \n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return predictions.astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d1b8e2b69d017f9deedf91c4b6c062dd","grade":true,"grade_id":"cell-8fb943ccb746dc45","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"nBQHTnaZeQFp","colab_type":"code","colab":{}},"source":["# Test the predict function\n","predictions = predict(parameters, X)\n","\n","assert predictions.shape == (X.shape[0], 1), \\\n","f\"Shape of predictions is {predictions.shape}, but it must be {(X.shape[0], 1)}\"\n","\n","# only labels 0, 1 are allowed\n","assert set(np.unique(predictions)).issubset(set([0, 1])), \\\n","f\"Predictions can only contain 0s and 1s but they contain {np.unique(predictions)}\"  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"b0657a4c3c4049d96ba31ca4e56820c2","grade":false,"grade_id":"cell-1777d7662b2646de","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wTDIFC98eQFs","colab_type":"code","colab":{}},"source":["@contract(Y_pred='array[Mx1],M>0',\n","          Y='array[Mx1],M>0',\n","          returns='float,>=0.0,<=1.0')\n","def calc_accuracy(Y_pred, Y):\n","    \"\"\"\n","    Calculates the accuracy of the predictions against the true labels\n","    (What percent of the predicted labels Y_pred matches the true labels in Y)\n","    \n","    param: Y_pred: Predictions of our model (numpy array of shape [m,1] containing 0s and 1s)\n","    param: Y: Target labels (numpy array of shape [m,output_dim])\n","    \n","    returns: accuracy (float between 0.0 and 1.0)\n","    \n","    \"\"\"\n","    \n","    accuracy = float((np.dot(Y, Y_pred.T) + np.dot(1-Y, 1-Y_pred.T))/float(Y.size)*100)\n","\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    \n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ee7a7624cb9b1afa7497a4755d50eb42","grade":true,"grade_id":"cell-ea608691f6aaf105","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"JZroMwq4eQFt","colab_type":"code","colab":{}},"source":["# Test the accuracy function\n","accuracy = calc_accuracy(A2, Y)\n","assert isinstance(accuracy, float) and accuracy >= 0.0 and accuracy <= 1.0, \\\n","\"Accuracy must be a float between 0.0 and 1.0\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"dff709123e90bbade436bc707d1ddf4c","grade":false,"grade_id":"cell-def0e0eabfe4d90c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"aNdG0_d4eQFv","colab_type":"text"},"source":["### Task 1.6: Numpy Network: Putting Everything Together\n","\n","The function `train_neural_network` uses batch gradient descent (entire training data is used at the same time to compute gradients) for training our numpy neural network. Complete this function by using the functions that you have implemented till now."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ee1fb840b5e2b5622d07bc07f761f661","grade":false,"grade_id":"cell-2368bee07b31960c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Zfwg_CExeQFv","colab_type":"code","outputId":"125c06ad-408a-4cf6-c264-d66fd9738c79","executionInfo":{"status":"error","timestamp":1587739344118,"user_tz":-120,"elapsed":1063,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["@contract(X_train='array[MxD],M>0,D>0',\n","          Y_train='array[Mx1],M>0',\n","          n_hidden='int,>0',\n","          learning_rate='float,>0.0',\n","          num_iterations='int,>0')\n","def train_neural_network(X_train, Y_train, n_hidden, learning_rate=0.01, num_iterations = 10000):\n","    \"\"\"\n","    Trains the network using batch gradient descent\n","    \n","    param: X_train: Training data (numpy array of shape [m,input_dim])\n","    param: Y_train: Training data labels (numpy array of shape [m,output_dim])\n","    param: n_hidden: Number of neurons in the hidden layer (int)\n","    param: num_iterations: Number of iterations in gradient descent loop (int)\n","    param: learning_rate: Learning rate for gradient descent (float)\n","    \n","    returns: parameters: Learned parameters - python dictionary containing W1, b1, W2 and b2\n","    \"\"\"\n","    losses = list()\n","    \n","    # Follow these steps:\n","    # Initialize parameters, then retrieve W1, b1, W2, b2.\n","    np.random.seed(3)\n","    n_x = layer_size(X_train,Y_train)[0]\n","    n_y = layer_size(X_train,Y_train)[2]\n","\n","    # initializing parameters: \n","    parameters = initialize_parameters(,n_hidden,n_y)\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","\n","    # For each iteration (till `num_iterations` is reached):\n","    # Loop (batch gradient descent) is a possible solution: \n","\n","    for i in range(0, num_iteration): \n","                       \n","        # Forward propagation: Compute A2, cache using `X_train` and `parameters`\n","        A2, cache = forward_propagation(X_train, parameters)\n","\n","        # Loss. Inputs: `A2`, `Y_train`, `parameters`. Outputs: `loss_value`. Append `loss_value` to `losses`\n","        loss_value = loss(A2, Y_train, parameters)\n","        losses.append(loss_value)\n","\n","        # Backpropagation. Inputs: `parameters`, `cache`, `X_train`, `Y_train`. Outputs: `grads`\n","        grads = backward_propagation(parameters, cache, X_train, Y_train)\n","\n","        # Parameter update. Inputs: `parameters`, `grads`, `learning_rate`. Outputs: `parameters`\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","            \n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    return parameters, losses"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e7acb66336b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m @contract(X_train='array[MxD],M>0,D>0',\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'array[Mx1],M>0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int,>0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float,>0.0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           num_iterations='int,>0')\n","\u001b[0;31mNameError\u001b[0m: name 'contract' is not defined"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"428fb8c0b52a6870aac23081f1e27612","grade":true,"grade_id":"cell-05507ffcc2ecfee0","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"HRVLpLRNeQFy","colab_type":"code","colab":{}},"source":["# Lets train a network with 3 units in the hidden layer using the moon dataset\n","params_moons_3, losses_moons_3 = train_neural_network(moons_x_train, \n","                                                      moons_y_train,\n","                                                      3, \n","                                                      learning_rate=0.01, \n","                                                      num_iterations=10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"140563d15b2c7ced2db57981a419a3f4","grade":false,"grade_id":"cell-555b024289338c11","locked":true,"schema_version":3,"solution":false,"task":false},"id":"r5LD7COBeQF1","colab_type":"code","colab":{}},"source":["def plot_decision_boundary_nn(ax, predict_fn, params, X_train, Y_train, X_test, Y_test, cmap='coolwarm'):\n","    \"\"\"\n","    Plots the decision boundary predicted by the neural network\n","    Don't worry about the details of this function\n","    \"\"\"\n","    \n","    markers = ('s', '^', 'x', 'o', 'v')\n","    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n","    cmap = ListedColormap(colors[:len(np.unique(Y_train))])\n","\n","    # For constructing the grid limits\n","    h = 0.02\n","    x_min, x_max = X_train[:,0].min() - 10*h, X_train[:,0].max() + 10*h\n","    y_min, y_max = X_train[:,1].min() - 10*h, X_train[:,1].max() + 10*h\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Make predictions for each value inside the grid and reshape\n","    Z = predict_fn(params, np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n","    cs = ax.contour(xx, yy, Z, colors='k', alpha=1.0)\n","    cs.collections[0].set_label(\"Decision boundary\")\n","    \n","    for idx, yv in enumerate(np.unique(Y_train[:,0])): \n","        ax.scatter(x=X_train[Y_train[:,0]==yv, 0], \n","                    y=X_train[Y_train[:,0]==yv, 1], \n","                    alpha=0.6, \n","                    c=[cmap(idx)], \n","                    marker=markers[0], \n","                    s=20,\n","                    label=f\"Train data class {yv}\",\n","                    edgecolors='k')\n","\n","    for idx, yv in enumerate(np.unique(Y_test[:,0])): \n","        ax.scatter(x=X_test[Y_test[:,0]==yv, 0], \n","                    y=X_test[Y_test[:,0]==yv, 1], \n","                    alpha=0.6, \n","                    c=[cmap(idx)], \n","                    marker=markers[1], \n","                    s=20,\n","                    label=f\"Test data class {yv}\",\n","                    edgecolors='k')\n","    ax.legend(ncol=2, fontsize=8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c67676a03eba7d4d34a84bffd1f6c7c4","grade":true,"grade_id":"cell-3fbed42a529284c2","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"KcdHyXwHeQF3","colab_type":"code","colab":{}},"source":["# Plot the decision boundary for network we just trained\n","fig, ax = plt.subplots(1,1,figsize=(8,5))\n","plot_decision_boundary_nn(ax, \n","                          predict, \n","                          params_moons_3, \n","                          moons_x_train, \n","                          moons_y_train, \n","                          moons_x_test, \n","                          moons_y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gi6NEkUKeQF5","colab_type":"code","colab":{}},"source":["# Now,let us train different networks with different number of hidden units\n","# If it takes too long, you can also try out values smaller than 100\n","\n","hidden = [5, 10, 30, 100]\n","params_moons_dict = dict()\n","\n","for h in hidden:\n","    print(f\"Training for hidden size {h}\")\n","    params, losses = train_neural_network(moons_x_train, \n","                                          moons_y_train, \n","                                          h, \n","                                          learning_rate=0.01, \n","                                          num_iterations=10000)\n","    \n","    train_acc = calc_accuracy(predict(params, moons_x_train), moons_y_train)\n","    test_acc = calc_accuracy(predict(params, moons_x_test), moons_y_test)\n","\n","    params_moons_dict[h] = {\"params\": params, \n","                            \"losses\": losses,\n","                            \"train_acc\": train_acc,\n","                            \"test_acc\": test_acc}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e046de5f55446c2a8acff6cb1fec08da","grade":false,"grade_id":"cell-ded18055a40b1f68","locked":true,"schema_version":3,"solution":false,"task":false},"id":"V1J8J75keQF6","colab_type":"code","colab":{}},"source":["# Plotting the decision boundaries for the different networks that were trained\n","\n","fig, ax = plt.subplots(2, 2, figsize=(9,7))\n","\n","for i, (h, value) in enumerate(params_moons_dict.items()):\n","    r = i//2\n","    c = i%2\n","    plot_decision_boundary_nn(ax[r][c],  \n","                              predict, \n","                              value[\"params\"], \n","                              moons_x_train, \n","                              moons_y_train, \n","                              moons_x_test, \n","                              moons_y_test)\n","    ax[r][c].set_title(f\"hidden size: {h}, test accuracy: {value['test_acc']:.2f}\",\n","                       fontsize=10)\n","plt.tight_layout()   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"54f548320310456dc6137a51a10da978","grade":false,"grade_id":"cell-3b924580fda942ad","locked":true,"schema_version":3,"solution":false,"task":false},"id":"a1eTzZTTeQF7","colab_type":"text"},"source":["If everything goes well, you should be able to observe that as the number of hidden units is increased, the decision boundary becomes highly non-linear. For high number of hiddent units, it can be seen that the classifier overfits the training data.\n","\n","**Hint**: If for some network, the decision boundary appears as a straight line, it may be because that network was not able to learn. Try changing the hyperparameters, such as the number of hidden units, the learning rate, or the number of iterations, and try again. You should be able to see plots similar to [this](https://ibb.co/ZY7G0wd)."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fa654439c28a203321108be4f44d177c","grade":false,"grade_id":"cell-c186cf2c1f3530e6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"L6h4FPxGeQF7","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","\n","## Part 2: PyTorch Basics <a name='pytorch_basics' />"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3b982a77e126f2a3ead69c428ccf2a8a","grade":false,"grade_id":"cell-370cdc2b30f1585e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"SWBhwNbzeQF8","colab_type":"text"},"source":["[PyTorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) is a Python-based scientific computing library that is commonly used for deep learning. It provides helpful features such as automatic differentitaion, the ability to utilize GPUs, and a large collection of deep-learning related algorithms. There are several other alternatives to PyTorch, such as Tensorflow or Caffe, but for this notebook, you will be using PyTorch. \n","\n","**If you are unfamiliar with PyTorch, we recommend you work through the [PyTorch 60minutes Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) first.** \n","\n","For the exercises below, no prior knowledge of PyTorch is assumed.\n","\n","Visit [this link](https://pytorch.org/tutorials/index.html) for some PyTorch tutorials."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"8b5a4e7c0106713b0cc6fe34823de881","grade":false,"grade_id":"cell-ec89345b52eade1f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"mgStjEuUeQF8","colab_type":"text"},"source":["First, let us use the same dataset that was used till now, but this time you will create a multilayered fully connected neural network using PyTorch for classifying the points of the dataset."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9701c9b2907d01a5b18dc545e9e82c0e","grade":false,"grade_id":"cell-bda54ed6ea6abb53","locked":true,"schema_version":3,"solution":false,"task":false},"id":"vbcylejueQF8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"6501baab-4dbe-481b-ddef-cfa8cb61f1c2","executionInfo":{"status":"error","timestamp":1587904616618,"user_tz":-120,"elapsed":4640,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchsummary import summary\n","\n","# Check https://pytorch.org/docs/stable/notes/randomness.html#reproducibility\n","torch.manual_seed(random_seed)\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7a00a7a26bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check https://pytorch.org/docs/stable/notes/randomness.html#reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'random_seed' is not defined"]}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f88b5599eb47d0d30cb95d7a63691bdc","grade":false,"grade_id":"cell-e25f9022224ef7dc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"rkJtWc8jeQF-","colab_type":"text"},"source":["### Task 2.1: PyTorch Basics: Network Definition\n","\n","Given below is the class definition for a fully connected network with 2 hidden layers and 1 output layer. Complete the missing parts. Comments are provided to guide you. **It may be helpful to get acquainted with the [different activation functions in PyTorch ](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).** The module is imported as `F`."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f3a9727732351572c46f65788ee4086c","grade":false,"grade_id":"cell-374daa750d40aeee","locked":false,"schema_version":3,"solution":true,"task":false},"id":"g6DlDXDPeQF-","colab_type":"code","outputId":"a1efddf4-2fef-4ed3-c496-200a20c86f1e","executionInfo":{"status":"error","timestamp":1587663301163,"user_tz":-120,"elapsed":1346,"user":{"displayName":"David Kurz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe_wM_K4xbhRICUKcX-SppDNFL1Q7WyvevxsgB=s64","userId":"17311286808322757220"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["class Net(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(Net, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.fc3 = nn.Linear(hidden_size, output_size)\n","\n","\n","\n","    def forward(self, x):\n","        # Follow these steps:\n","        #\n","        # Flatten the input x keeping the batch dimension the same\n","        x = self.conv(x)\n","        x = x.view(x.size(0), -1) #keep batch size\n","\n","        # Use the relu activation on the output of self.fc1(x)\n","        x = F.relu(self.fc1(x))\n","\n","        # Use the relu activation on the output of self.fc2(x)\n","        x = F.relu(self.fc2(x))\n","        \n","        # Pass x through fc3 but do not apply any activation function (why not?)\n","        # cause it's not the actual task..neuro network doesn't know anything \n","        x = self.fc3(x)\n","        \n","        # x = F.log_softmax(x, dim=1)\n","\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","        return x  # Return x (logits)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4879ae8c1cd5>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    def forward(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6d3fc0382d27214421aad86eaf56a955","grade":true,"grade_id":"cell-d9a897be467d6623","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"60WwkOS4eQF_","colab_type":"code","colab":{}},"source":["# Verify the network class\n","testnet = Net(2, 20, 1)\n","ip = torch.rand((350,2))\n","# This is not the best way to pass the data through the net\n","# In the next task you will need to find the better way\n","op = testnet.forward(ip)  \n","assert op.shape==(350,1), f\"Output shape must be (350,1) but it is {op.shape}\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2dcf87b01ed2ce0ef660c40059aa4851","grade":false,"grade_id":"cell-18a5d36a39c028bd","locked":true,"schema_version":3,"solution":false,"task":false},"id":"u2iqWZQOeQGB","colab_type":"text"},"source":["### Task 2.2: PyTorch Basics: Training\n","\n","Once the network is defined, it can be trained by following roughly the same sequence of steps as the numpy network you trained earlier. This is done in the function `train_neural_network_pytorch` given below. Complete the missing parts of this function.\n","\n","**Note**: \n","* The weights and biases of the network are initialized when the network object is created (outside the training function)\n","* PyTorch operates on [Tensors](https://pytorch.org/docs/stable/tensors.html) which are multi-dimensional matrices containing elements of a single data type. Although tensors look similar to numpy arrays, they have several additional properties. Therefore, any data or labels that are used with a PyTorch network need to be converted into Tensors. \n","* PyTorch includes a large number of training algorithms (optimizers) for training networks. See [here for details of how to use an optimizer](https://pytorch.org/docs/stable/optim.html) and their different types. The code for using an optimizer for this notebook is provided to you.\n","* Several types of loss functions (**criterion**) [are also available](https://pytorch.org/docs/stable/nn.html#loss-functions)."]},{"cell_type":"code","metadata":{"id":"IhtkFcnieQGB","colab_type":"code","colab":{}},"source":["# Define hyperparameters\n","LEARNING_RATE = 0.001\n","MOMENTUM = 0.9\n","MAX_ITERATIONS = 10000\n","INPUT_SIZE = 2\n","HIDDEN_SIZE = 20\n","OUTPUT_SIZE = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"52316f891188e7269c63446f697b0ba5","grade":false,"grade_id":"cell-f455dfb1b6d2d156","locked":true,"schema_version":3,"solution":false,"task":false},"id":"KBmH7sTUeQGD","colab_type":"text"},"source":["We use the [BCEWithLogitsLoss function](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss) so familiarize yourself with this type of function."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"af730be15243b308ad5b2c456bf857a7","grade":false,"grade_id":"cell-02867bd47b3d3971","locked":true,"schema_version":3,"solution":false,"task":false},"id":"cjESrtMzeQGE","colab_type":"code","colab":{}},"source":["# Initialize the network\n","net = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n","\n","# Define the loss criterion and the training algorithm\n","criterion = nn.BCEWithLogitsLoss()  # Be careful, use binary cross entropy for binary, CrossEntropy for Multi-class\n","optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2cc8187b0467a975910ba53d740383c1","grade":false,"grade_id":"cell-dc9674310f82862f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"RDaSTzEYeQGG","colab_type":"code","colab":{}},"source":["def train_neural_network_pytorch(net, inputs, labels, optimizer, criterion, iterations=10000):\n","    \"\"\"\n","    :param net: the neural network object\n","    :param inputs: numpy array of training data values\n","    :param labels: numpy array of training data labels \n","    :param optimizer: PyTorch optimizer instance\n","    :param criterion: PyTorch loss function\n","    :param iterations: number of training steps\n","    \"\"\"\n","    net.train()  # Before training, set the network to training mode\n","\n","    for iter in trange(iterations):  # loop over the dataset multiple times\n","\n","        # It is a common practice to track the losses during training\n","        runnig_loss = 0.0\n","        # Feel free to do so if you want\n","        \n","        # Get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # Convert to tensors if data is in the form of numpy arrays\n","        if not torch.is_tensor(inputs):\n","            inputs = torch.from_numpy(inputs.astype(np.float32)) \n","            \n","        if not torch.is_tensor(labels):\n","            labels = torch.from_numpy(labels.astype(np.float32))\n","\n","            \n","        # Follow these steps:\n","        # 1. Reset gradients: Zero the parameter gradients (Check the link for optimizers in the text cell above to find the correct function)\n","        optimizer.zero_grad()\n","\n","        # 2. Forward: Pass `inputs` through the network. This can be done calling the `forward` function of `net` explicitly but there is an \n","        # easier way that is more commonly used\n","        outputs = net(inputs)\n","\n","        # 3. Compute the loss: Use `criterion` and pass it the `outputs` and `labels`\n","        # Check the link in the text cell above for details\n","        loss = criterion(outputs, labels)\n","\n","        # 4. Backward: Call the `backward` function in `loss`\n","        loss.backward()\n","\n","        # 5. Update parameters: This is done using the optimizer's `step` function. \n","        # Check the link provided for details.\n","        optimizer.step()\n","\n","        # print statistics as a common practice to track losses during training: \n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","        \n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","        \n","    print('Finished Training')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3778d2ab09c6ad3d7bc0bdae3e371c5c","grade":false,"grade_id":"cell-8750300a00fba32e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"jJQxCNFoeQGI","colab_type":"text"},"source":["### Task 2.3: PyTorch Basics: Activation Function after Final FC layer output\n","\n","In the `forward` function of the `Net` class, we did not use any non-linear activation function on the final output. Why?"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"154c264e9ca919bfad43113f61173d9b","grade":true,"grade_id":"cell-0012f8fddb61492e","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"Eo90tI0PeQGI","colab_type":"text"},"source":["YOUR ANSWER HERE"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e84716c497fdfbe086767bbeb4e2c399","grade":false,"grade_id":"cell-be1360c34b680875","locked":true,"schema_version":3,"solution":false,"task":false},"id":"o67JkkJXeQGM","colab_type":"code","colab":{}},"source":["# Check the network stats\n","summary(net, input_size=(2,), device=\"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4f141c5f07c7c4ff7ed0c127b4c5b891","grade":false,"grade_id":"cell-10455484bf80e2e8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Ul9pKmtreQGN","colab_type":"text"},"source":["### Task 2.4: PyTorch Basics: Verify Number of Trainable Parameters\n","\n","The `summary` function from `torchsummary` is useful for finding the statistics of network parameters layers and output shapes. Calculate the number of parameters for each layer ($W$ and $b$ separately) by hand and verify your result with the number of parameters shown in the cell above.\n","\n","How many parameters are there?\n","\n"," - Layer 1 (first hidden layer): Weights = ...\n"," - Layer 1 (first hidden layer): Bias = ...\n"," ...\n","\n","Total = ?"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"d201438c040cdf5fddac7b2f1be72260","grade":true,"grade_id":"cell-2a87fb8c930084e6","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"jgEXy62heQGO","colab_type":"text"},"source":["YOUR ANSWER HERE"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"984bc363730f6800882f0c91897130bb","grade":true,"grade_id":"cell-5ecc6e755af39b86","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"eHhAVCjleQGP","colab_type":"code","colab":{}},"source":["# Train the PyTorch network\n","train_neural_network_pytorch(net, moons_x_train, moons_y_train, optimizer, criterion, MAX_ITERATIONS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"38296e9e88ec5d9c4c9be13c685273c3","grade":false,"grade_id":"cell-c059b40082258785","locked":true,"schema_version":3,"solution":false,"task":false},"id":"BSyxDn73eQGR","colab_type":"code","colab":{}},"source":["def predict_pytorch(net, X):\n","    \"\"\"\n","    Function for producing network predictions\n","    \"\"\"\n","    \n","    net.eval()\n","    \n","    # Make predictions (class 0 or 1) using the learned parameters\n","    \n","    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n","    X = torch.from_numpy(X.astype(np.float32))\n","    logits = net(X)\n","    predictions = torch.sigmoid(logits) > 0.5\n","    \n","    return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e66dd7da76cd32b8cea700f32b8000ce","grade":true,"grade_id":"cell-d0f2991ef4efc0b3","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"TB_EFM3DeQGT","colab_type":"code","colab":{}},"source":["# Plot the decision boundary learned by the neural network\n","fig, ax = plt.subplots(1,1,figsize=(8,5))\n","plot_decision_boundary_nn(ax, \n","                          predict_pytorch, \n","                          net, \n","                          moons_x_train, \n","                          moons_y_train, \n","                          moons_x_test, \n","                          moons_y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d16518ae9e30c3b961347d355be7b555","grade":true,"grade_id":"cell-94b1811c94eb1bf7","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"JbJY_kXKeQGX","colab_type":"code","colab":{}},"source":["# Calculate the accuracies on the training and test data\n","# You should be able to get accuracies around 0.9\n","train_acc = calc_accuracy(predict_pytorch(net, moons_x_train).data.numpy(), moons_y_train)\n","test_acc = calc_accuracy(predict_pytorch(net, moons_x_test).data.numpy(), moons_y_test)\n","print(f\"Train accuracy: {train_acc:.2f}, Test accuracy: {test_acc:.2f}\")\n","\n","assert train_acc>0.8 and test_acc>0.8, \"Rerun with different hyperparameters to get better accuracies\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JFmEUYseQGa","colab_type":"code","colab":{}},"source":["# Let us again train different networks with different number of hidden units\n","# You can also experiment with larger sizes for the hidden units by using bigger numbers in the `hidden` list\n","\n","hidden = [10, 20, 30, 50]\n","pytorch_moons_dict = dict()\n","\n","for h in hidden:\n","    net = Net(INPUT_SIZE, h, OUTPUT_SIZE)\n","    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n","\n","    print(f\"Training for hidden size {h}\")\n","    \n","    train_neural_network_pytorch(net, \n","                                 moons_x_train, \n","                                 moons_y_train, \n","                                 optimizer, \n","                                 criterion, \n","                                 iterations=10000)\n","    \n","    train_acc = calc_accuracy(predict_pytorch(net, moons_x_train).data.numpy(), moons_y_train)\n","    test_acc = calc_accuracy(predict_pytorch(net, moons_x_test).data.numpy(), moons_y_test)\n","\n","    pytorch_moons_dict[h] = {\"net\": net, \n","                             \"train_acc\": train_acc,\n","                             \"test_acc\": test_acc}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8b68b1892331eb4cb6dd46512e1f66c6","grade":false,"grade_id":"cell-b0114c0ce1c6addf","locked":true,"schema_version":3,"solution":false,"task":false},"id":"_y9KLBDheQGc","colab_type":"code","colab":{}},"source":["# Plot decision boundaries for the different networks\n","\n","fig, ax = plt.subplots(2, 2, figsize=(9,7))\n","for i, (h, value) in enumerate(pytorch_moons_dict.items()):\n","    r = i//2\n","    c = i%2\n","    plot_decision_boundary_nn(ax[r][c], \n","                              predict_pytorch, \n","                              value[\"net\"], \n","                              moons_x_train, \n","                              moons_y_train, \n","                              moons_x_test, \n","                              moons_y_test)\n","    ax[r][c].set_title(f\"hidden size: {h}, test accuracy: {value['test_acc']:.2f}\",\n","                       fontsize=10)\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4cd87dad6185fde9e2de2417bf02b9bb","grade":false,"grade_id":"cell-30a463ad0c581a2f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"85-BGl74eQGe","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","## Part 3: Multiclass Classification with PyTorch <a name='multi' />"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"192b2a2b1d1f670c0882ac9554440770","grade":false,"grade_id":"cell-e390748b27e1ba5a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1eH9d72leQGf","colab_type":"text"},"source":["Till now, we had been training networks for performing binary classification. Now, we move on to the problem of multiclass classification, where for a given input, the network needs to predict one out of $C$ possible classes. Additionally, till now, we had been training the network using batch gradient descent, where the entire training dataset is used to compute gradients in each iteration. Now, we will use minibatch gradient descent, where a minibatch (smaller subset of the train set) is used in each iteration."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2af522f68cfa1e7b46cb14758cc33c5c","grade":false,"grade_id":"cell-74752484b26b4c8b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Ig1egpxWeQGf","colab_type":"code","colab":{}},"source":["from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f84c3561c82eb7222864894fe785aee8","grade":false,"grade_id":"cell-6a257a2d86b39c6e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"jLRSmQzTeQGh","colab_type":"text"},"source":["For this part of the notebook, and the next, we will be using the [MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist) dataset from [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html). First, we need to cteate dataset objects for the training and test data. The first time the next cell is run, the data is downloaded and stored in the `data` directory. Standard transformations are applied to the data tenstors."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"28f2aa70abe5bdb62319121b5918ef41","grade":false,"grade_id":"cell-ea60a53f3393e14a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"gZM2Wi_2eQGi","colab_type":"code","colab":{}},"source":["train_dataset = datasets.MNIST('./data', \n","                               train=True, \n","                               download=running_local,\n","                               transform=transforms.Compose([transforms.ToTensor(),\n","                                                             transforms.Normalize((0.1307,), (0.3081,))\n","                                                            ]))\n","\n","test_dataset = datasets.MNIST('./data', \n","                              train=False, \n","                              transform=transforms.Compose([transforms.ToTensor(),\n","                                                            transforms.Normalize((0.1307,), (0.3081,))\n","                                                           ]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"24257c5618c1aa6dab1cdc7fd395aa9f","grade":false,"grade_id":"cell-5fb294dd698ab0e6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7pt49GUreQGk","colab_type":"code","colab":{}},"source":["# View sample data and targets\n","fig, ax = plt.subplots(1,5,figsize=(10,2))\n","for i in range(5):\n","    ax[i].imshow(train_dataset[i][0].data.numpy().squeeze(0), cmap='gray')\n","    ax[i].set_title(f\"Label: {train_dataset[i][1]}\")\n","    ax[i].axis('off')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1CtDei5eQGo","colab_type":"code","colab":{}},"source":["# Define hyperparameters\n","LEARNING_RATE = 0.001\n","MOMENTUM = 0.9\n","NUM_EPOCHS = 3\n","HIDDEN_SIZE = 200\n","TRAIN_BATCH_SIZE = 100\n","TEST_BATCH_SIZE = 100\n","INPUT_SIZE = 784  # Do not change this\n","HIDDEN_SIZE = 200\n","OUTPUT_SIZE = 10  # Do not change this"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"79931065d8229ca8468705572736ff54","grade":false,"grade_id":"cell-df170309508271fa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"_p5aEVT9eQGr","colab_type":"text"},"source":["### Task 3.1: Multiclass Classification: Using DataLoaders\n","\n","For training using minibatches, PyTorch provides the [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) utility which combines a dataset and a sampler, and provides an iterable over the given dataset. Once a data loader is created, it is really easy to fetch a minibatch of data from it in the actual training loop. Complete the missing parts in the code cell below to create the train and test data loaders."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"979b206fe0aa77675123dd474b8e417a","grade":false,"grade_id":"cell-08f90dca0237de35","locked":false,"schema_version":3,"solution":true,"task":false},"id":"f82fprCveQGr","colab_type":"code","colab":{}},"source":["# Create the train data loader\n","#train_loader = torch.utils.data.DataLoader(dataset= ,           # Pass the appropriate dataset\n","#                                           batch_size= ,        # Use the correct batch_size \n","#                                           shuffle=True,           # Shuffling the data is a good idea\n","#                                           drop_last= )         # Ignore the last minibatch if its not full\n","\n","# Create the test data loader in the same way\n","# Take care of using the correct dataset and batch size\n","#test_loader = torch.utils.data.DataLoader(dataset= ,            # Pass the appropriate dataset\n","#                                          batch_size= ,         # Use the correct batch_size \n","#                                          shuffle=True,            # Shuffling the data is a good idea\n","#                                          drop_last= )          # Ignore the last minibatch if its not full\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3536ed1278c61e6d0f2bf0d646fa06ef","grade":false,"grade_id":"cell-7c478d22bd75f4e5","locked":true,"schema_version":3,"solution":false,"task":false},"id":"E_H9NdKheQGt","colab_type":"text"},"source":["### Task 3.2: Multiclass Classification: Number of Minibatches\n","\n","Find the number of minibatches in the train and test data loaders."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0f538d0438b8180e6c30097e05c1a8c9","grade":false,"grade_id":"cell-eb913b0d9826a877","locked":false,"schema_version":3,"solution":true,"task":false},"id":"ax5x4182eQGt","colab_type":"code","colab":{}},"source":["# How many minibatches in train and test data loader Use variables `num_train_batches` and `num_test_batches`\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","\n","print(f\"num_train_batches: {num_train_batches}, num_test_batches: {num_test_batches}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2999c9d385f6529ba2f63b31da24ebf1","grade":false,"grade_id":"cell-b729acee6c92fdba","locked":true,"schema_version":3,"solution":false,"task":false},"id":"whhdNmZreQGw","colab_type":"text"},"source":["### Multiclass Classification: Cross Entropy Loss"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f98e48adcfc763f29ff53d6488f375ff","grade":false,"grade_id":"cell-73bc5dea35864a5a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2sSOqsP_eQGx","colab_type":"code","colab":{}},"source":["# Instantiate the network and set up the loss function and optimizer\n","net_mnist = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n","\n","criterion = nn.CrossEntropyLoss()  # Be careful, use BCE for binary, CrossEntropy for Multi-class\n","optimizer = optim.SGD(net_mnist.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1cb8e7898a76913065d0b752565c1e14","grade":false,"grade_id":"cell-8811a271c702d574","locked":true,"schema_version":3,"solution":false,"task":false},"id":"awxnbDN_eQG6","colab_type":"code","colab":{}},"source":["# Check the parameter statistics\n","summary(net_mnist, input_size=(INPUT_SIZE,), device=\"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"16999f37b697ee749cdfc86092357611","grade":false,"grade_id":"cell-5c8615adb99d638d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ZSI6qs0teQG9","colab_type":"text"},"source":["### Task 3.3: Multiclass Classification: Training\n","\n","In the function `train_neural_network_pytorch_minibatch`, minibatch gradient descent is to be used. Complete the missing parts in the function, so that a minibatch of data can be loaded using the `train_loader`. Check [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network) for a possible way of using a data loader. Once the minibatch is loaded, complete the steps indicated by the comments (same as before)."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"42bd26419704c9c666b81581aa140644","grade":false,"grade_id":"cell-e8a6b1c847e1c08a","locked":false,"schema_version":3,"solution":true,"task":false},"id":"ZR0I9IbTeQG9","colab_type":"code","colab":{}},"source":["def train_neural_network_pytorch_minibatch(net, train_loader, optimizer, criterion, num_epochs):\n","\n","    net.train()  # Set the network in training mode\n","\n","    for epoch in range(num_epochs):\n","        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n","            # data is a batch of feature sets: \n","            # Follow these steps inside the loop here:\n","            # 1. Zero parameter gradients\n","            net.zero_grad()\n","            # 2. Forward\n","            output = net(data)\n","            # 3. Compute loss\n","            loss = F.nll(output,target)\n","            # 4. Backward\n","            loss.backward()\n","            # 5. Update step\n","            optimizer.step()\n","\n","            # YOUR CODE HERE\n","            raise NotImplementedError()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d346b27fb678453a3d3cf8acb1d6bf11","grade":false,"grade_id":"cell-85f9c9f5cba1e7d9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"bL6F6LkVeQHC","colab_type":"code","colab":{}},"source":["def calc_accuracy_minibatch(net, data_loader):\n","    \"\"\"\n","    Calculates the overall accuracy by using minibatches\n","    \"\"\"\n","    net.eval()\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in data_loader:\n","            output = net(data)\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    accuracy = correct/len(data_loader.dataset)\n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a0178fa72ed7858bf2a9b2643dcf1988","grade":true,"grade_id":"cell-98200547440fe423","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"4n_d0G7MeQHD","colab_type":"code","colab":{}},"source":["train_neural_network_pytorch_minibatch(net_mnist, train_loader, optimizer, criterion, NUM_EPOCHS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e55cc91604e67a3499e2f08663664ccf","grade":true,"grade_id":"cell-7eeed8cadd528955","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"k-ykXZw2eQHG","colab_type":"code","colab":{}},"source":["# Check the train and test accuracies\n","train_accuracy = calc_accuracy_minibatch(net_mnist, train_loader)\n","test_accuracy = calc_accuracy_minibatch(net_mnist, test_loader)\n","print(f\"Train accuracy: {train_accuracy: .2f}, Test accuracy: {test_accuracy: .2f}\")\n","\n","assert train_accuracy>0.85 and test_accuracy>0.85, \\\n","\"Rerun with different hyperparameters to get better accuracies\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"7df8eedb11fdf3784cd83d36ab9d0370","grade":false,"grade_id":"cell-320c4f34759fd343","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8N2cWvpIeQHJ","colab_type":"text"},"source":["<a href=#objectives> [go to top] </a>\n","## Part 4: Convolutional Neural Networks with PyTorch <a name='cnn' />"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2a624e84eacffad44075569a67b33a12","grade":false,"grade_id":"cell-e0c74080b5a056f2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"vFvXTavTeQHJ","colab_type":"text"},"source":["In this section we shall see the usual steps of training a CNN. \n","\n","However, this is a very brief section, and you are encouraged to check out [this tutorial on CNN with PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network). Look here for [more information on CNNs and accessible illustrations on how CNNs work](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks). For [CNN layers in PyTorch see here](https://pytorch.org/docs/stable/nn.html#convolution-layers)."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2cdb5556e08f104247fe8ef888a40725","grade":false,"grade_id":"cell-a4b19370764f145a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"aLU3veHteQHK","colab_type":"text"},"source":["### Task 4.1: CNN: Network Definition\n","\n","Complete the class definition of a CNN in the code cell below. Find and use the appropriate 2D convolutional layer."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f5b1f3b2f2c23585bd72b5b823472120","grade":false,"grade_id":"cell-5313bf814b7ee9d7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"KCC0OCneeQHK","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","    def __init__(self, output_size):\n","        super(CNN, self).__init__()\n","               \n","        # Set self.conv1 = A suitable 2D Conv layer\n","        # Use a 2D Conv layer with 1 input channel, 32 output channels, filter size of 3 and stride of 1\n","\n","\n","        \n","        # Set self.conv2 = A suitable 2D Conv layer\n","        # Use a 2D Conv layer with 32 input channel, 64 output channels, filter size of 3 and stride of 1\n","\n","        \n","        \n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","        \n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, output_size)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"56dd64c143cdf4fc682be61ce4c80273","grade":false,"grade_id":"cell-f208585cbda655d8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"UGNyjS2ieQHM","colab_type":"code","colab":{}},"source":["# Define hyperparameters\n","LEARNING_RATE = 0.001\n","MOMENTUM = 0.9\n","NUM_EPOCHS = 3\n","TRAIN_BATCH_SIZE = 100\n","TEST_BATCH_SIZE = 100\n","OUTPUT_SIZE = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0234d90e24c67a8cc4704830775bb869","grade":false,"grade_id":"cell-eb957c79f53a0d1c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"HQjFnJRVeQHO","colab_type":"code","colab":{}},"source":["# Instantiate the CNN and set up the loss and optimizer\n","net_cnn = CNN(OUTPUT_SIZE)\n","\n","criterion = nn.CrossEntropyLoss()  # Be careful, use BCE for binary, CrossEntropy for Multi-class\n","optimizer = optim.SGD(net_cnn.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"351e2b679782f5e5e8774a4d1d6ff195","grade":false,"grade_id":"cell-b7ca0b5a52cf8afa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"dpbk3KleeQHS","colab_type":"code","colab":{}},"source":["# Check the number of parameters\n","summary(net_cnn, input_size=(1,28,28), device=\"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a64be2eb10ea2503cf3470fc1b659b51","grade":false,"grade_id":"cell-86c199fe95e27ace","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NIGvfqyAeQHT","colab_type":"text"},"source":["**Note**: The bulk of the parameters in a CNN reside in the fully connected layers. The convolutional layers use shared weights in the kernels (filters) and hence have a much lower number of parameters."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8d0119eb086f5eace7a471114f620151","grade":true,"grade_id":"cell-80d1c053096eb1d5","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"rhmY3SHueQHT","colab_type":"code","colab":{}},"source":["# Train the CNN using the minibatch training function defined earlier\n","train_neural_network_pytorch_minibatch(net_cnn, train_loader, optimizer, criterion, NUM_EPOCHS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fff3961736fb8ec8b385c9d484b6b8df","grade":true,"grade_id":"cell-07b4e91d9c096359","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"ISfte5s7eQHU","colab_type":"code","colab":{}},"source":["# Check the train and test accuracy\n","train_accuracy = calc_accuracy_minibatch(net_cnn, train_loader)\n","test_accuracy = calc_accuracy_minibatch(net_cnn, test_loader)\n","print(f\"Train accuracy: {train_accuracy: .2f}, Test accuracy: {test_accuracy: .2f}\")\n","\n","assert train_accuracy>0.9 and test_accuracy>0.9, \\\n","\"Rerun with different hyperparameters to get better accuracies\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c28d859472c732fe36d3da5efb6e87bf","grade":false,"grade_id":"cell-3fea6a6e4cc43d49","locked":true,"schema_version":3,"solution":false,"task":false},"id":"GB4OumGReQHV","colab_type":"text"},"source":["### Task 4.2: CNN: Question about Performance\n","\n","What can be done to improve the performance of the CNN you just trained and evaluated?"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"abadad027e0d02c4989567fdb6bb487b","grade":true,"grade_id":"cell-2d7e7b216d3b5418","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"5A_J5a0SeQHW","colab_type":"text"},"source":["YOUR ANSWER HERE"]}]}